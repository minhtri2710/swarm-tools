{"id":"opencode-swarm-monorepo-lf2p4u-mjepneb3e6h","title":"Fix remaining 9 test failures","description":"Three distinct issue groups:\n\n1. **swarm_decompose/plan_prompt** (3 failures) - Prompt content assertions failing. Tests expect certain content in generated prompts that isn't there.\n\n2. **Checkpoint/Recovery** (3 failures) - \"there is no unique or exclusion constraint matching the ON CONFLICT specification\" - The swarm_contexts table is missing a unique constraint needed for UPSERT operations.\n\n3. **FTS search** (1 failure) - Full-text search fallback test failing. Likely missing FTS5 virtual table or triggers in test setup.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-20T19:48:02.991Z","updated_at":"2025-12-20T20:02:34.360Z","closed_at":"2025-12-20T20:02:34.360Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjepneb7ybq","title":"Fix swarm_decompose and swarm_plan_prompt tests","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-20T19:48:02.995Z","updated_at":"2025-12-20T20:02:27.546Z","closed_at":"2025-12-20T20:02:27.546Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjepneb3e6h","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjepneba91m","title":"Fix checkpoint/recovery - add unique constraint to swarm_contexts","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-20T19:48:02.998Z","updated_at":"2025-12-20T20:02:29.114Z","closed_at":"2025-12-20T20:02:29.114Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjepneb3e6h","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjepnebcihu","title":"Fix FTS search fallback test","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-20T19:48:03.000Z","updated_at":"2025-12-20T20:02:30.940Z","closed_at":"2025-12-20T20:02:30.940Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjepneb3e6h","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9s8wtmt1","title":"Test cell from debug session","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T05:11:41.597Z","updated_at":"2025-12-21T05:13:10.413Z","closed_at":"2025-12-21T05:13:10.413Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9ovwf","title":"Migration runner with schema validation","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T05:17:13.836Z","updated_at":"2025-12-21T05:41:15.798Z","closed_at":"2025-12-21T05:41:15.798Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9zuyq","title":"Integration tests: hive_* tools (9 tools)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T05:17:13.847Z","updated_at":"2025-12-21T05:41:40.541Z","closed_at":"2025-12-21T05:41:40.541Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zda165l","title":"Integration tests: agentmail_* tools (11 tools)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T05:17:13.849Z","updated_at":"2025-12-21T05:41:41.871Z","closed_at":"2025-12-21T05:41:41.871Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zda2hg5","title":"Integration tests: semantic_memory_* tools (8 tools)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T05:17:13.850Z","updated_at":"2025-12-21T05:41:43.066Z","closed_at":"2025-12-21T05:41:43.066Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zda4z0b","title":"Integration tests: swarm_* tools (24 tools)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T05:17:13.852Z","updated_at":"2025-12-21T05:41:44.165Z","closed_at":"2025-12-21T05:41:44.165Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zda632z","title":"Integration tests: skills_* tools (9 tools)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T05:17:13.854Z","updated_at":"2025-12-21T05:41:16.982Z","closed_at":"2025-12-21T05:41:16.982Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zda7324","title":"Integration tests: structured_* + mandate_* tools (10 tools)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T05:17:13.855Z","updated_at":"2025-12-21T05:41:18.306Z","closed_at":"2025-12-21T05:41:18.306Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zda9lvj","title":"Integration tests: repo_* tools (5 tools)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T05:17:13.857Z","updated_at":"2025-12-21T05:41:19.312Z","closed_at":"2025-12-21T05:41:19.312Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9rprs","title":"Convert streams subsystem to Drizzle queries","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T05:17:13.839Z","updated_at":"2025-12-21T16:21:02.703Z","closed_at":"2025-12-21T16:21:02.703Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9uul8","title":"Convert memory subsystem to Drizzle queries","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T05:17:13.842Z","updated_at":"2025-12-21T16:21:03.936Z","closed_at":"2025-12-21T16:21:03.936Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9vceg","title":"Convert hive subsystem to Drizzle queries","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T05:17:13.843Z","updated_at":"2025-12-21T17:24:31.341Z","closed_at":"2025-12-21T17:24:31.341Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2wbk8","title":"Port DurableLock to libSQL","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T16:21:21.704Z","updated_at":"2025-12-21T16:27:15.787Z","closed_at":"2025-12-21T16:27:15.787Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2zg9f","title":"Port DurableDeferred to libSQL","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T16:21:21.707Z","updated_at":"2025-12-21T16:37:40.796Z","closed_at":"2025-12-21T16:37:40.796Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg31hwz","title":"Port DurableCursor to libSQL","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T16:21:21.709Z","updated_at":"2025-12-21T16:37:42.151Z","closed_at":"2025-12-21T16:37:42.151Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg33y71","title":"Port DurableMailbox and ask pattern to libSQL","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T16:21:21.711Z","updated_at":"2025-12-21T16:48:17.096Z","closed_at":"2025-12-21T16:48:17.096Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg35hv2","title":"Remove PGLite from streams/index.ts exports","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T16:21:21.713Z","updated_at":"2025-12-21T17:24:33.638Z","closed_at":"2025-12-21T17:24:33.638Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfznai9mco","title":"Add Drizzle Kit migrations","description":"Use drizzle-kit for proper schema migrations instead of manual CREATE TABLE IF NOT EXISTS.\n\nBenefits:\n- Automatic migration generation from schema changes\n- Version tracking\n- Rollback support\n- Type-safe migrations\n\nWould replace the current ad-hoc migration logic in libsql-schema.ts.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-21T17:15:40.401Z","updated_at":"2025-12-21T17:15:40.401Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfzgw9c7gd","title":"Fix: SQLITE_ERROR no such column: stream in hive_create_epic","description":"When calling hive_create_epic, get error: SQLITE_ERROR: no such column: stream\n\nContext:\n- Happens when creating epic in a project\n- swarmmail_init may be reusing wrong project context\n- Schema migration may not have run on hive database\n\nLikely causes:\n1. Schema migration not run on hive SQLite database\n2. Wrong database being accessed (project path mismatch)\n3. cursors table schema change (stream column) not applied to existing DBs\n\nRelated to PGLite→libSQL migration - the cursors table schema changed from stream_id to stream column.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-21T17:10:42.000Z","updated_at":"2025-12-21T17:26:06.716Z","closed_at":"2025-12-21T17:26:06.716Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg00gnvf8y","title":"Fix P0: cursors table schema migration (stream column)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T17:25:54.907Z","updated_at":"2025-12-21T17:32:38.681Z","closed_at":"2025-12-21T17:32:38.681Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg00gnmwui","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg00go0fga","title":"Fix agentmail_release integration tests (3 failing)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T17:25:54.912Z","updated_at":"2025-12-21T17:37:03.773Z","closed_at":"2025-12-21T17:37:03.773Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg00gnmwui","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg00go89jn","title":"Fix swarm_checkpoint integration tests (2 failing)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T17:25:54.920Z","updated_at":"2025-12-21T17:37:06.462Z","closed_at":"2025-12-21T17:37:06.462Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg00gnmwui","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg01a7xgtu","title":"Bug: hive_update cannot close cells (CHECK constraint fails)","description":"When calling hive_update with status='closed', get error:\n\n```\nSQLITE_CONSTRAINT_CHECK: CHECK constraint failed: (status = 'closed') = (closed_at IS NOT NULL)\n```\n\nRoot cause: The beads table has a CHECK constraint requiring closed_at to be set when status='closed'. hive_update doesn't set closed_at when changing status to closed.\n\nFix: Either:\n1. hive_update should set closed_at = NOW() when status='closed'\n2. Or hive_update should reject status='closed' and require hive_close instead\n\nLocation: packages/swarm-mail/src/hive/adapter.ts (updateCell method)","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-21T17:26:33.213Z","updated_at":"2025-12-21T17:32:40.860Z","closed_at":"2025-12-21T17:32:40.860Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg00gnmwui","title":"Fix P0 Schema Bug + Integration Tests + Effect Primitives","description":"Sequential risk-based approach: Fix cursors schema migration bug (P0), then fix 6 failing integration tests, consolidate duplicate schemas, and integrate DurableLock/DurableDeferred into swarm coordination.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T17:25:54.898Z","updated_at":"2025-12-21T17:48:25.022Z","closed_at":"2025-12-21T17:48:25.022Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg00goagh1","title":"Consolidate duplicate schema definitions","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:25:54.922Z","updated_at":"2025-12-21T17:45:23.718Z","closed_at":"2025-12-21T17:45:23.718Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg00gnmwui","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg00god17i","title":"Port DurableLock to libSQL/DatabaseAdapter","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:25:54.925Z","updated_at":"2025-12-21T17:48:17.068Z","closed_at":"2025-12-21T17:48:17.068Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg00gnmwui","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg00gogwct","title":"Port DurableDeferred to libSQL/DatabaseAdapter","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:25:54.928Z","updated_at":"2025-12-21T17:48:18.268Z","closed_at":"2025-12-21T17:48:18.268Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg00gnmwui","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","title":"Drizzle Migration + Plugin Integration Tests","description":"Convert all raw SQL to Drizzle ORM with proper migrations. Add happy-path integration tests for all 93 plugin tools. TDD approach: test first, then implement.\n\nBranch: feat/drizzle-migration-and-tests\n\n## Phase 1: Foundation (sequential)\n- Migration runner with schema validation\n\n## Phase 2: Drizzle Conversion (parallel after Phase 1)\n- Convert streams subsystem\n- Convert memory subsystem  \n- Convert hive subsystem\n\n## Phase 3: Cleanup (after Phase 2)\n- Remove duplicate schema definitions\n\n## Phase 4: Integration Tests (parallel, some depend on Phase 2)\n- hive_* tools (9)\n- agentmail_* tools (11)\n- semantic_memory_* tools (8)\n- swarm_* tools (24)\n- skills_* tools (9)\n- structured_* + mandate_* tools (10)\n- repo_* tools (5)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T05:17:13.832Z","updated_at":"2025-12-21T18:01:12.967Z","closed_at":"2025-12-21T18:01:12.967Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9xs46","title":"Remove duplicate schema definitions","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T05:17:13.845Z","updated_at":"2025-12-21T18:01:04.308Z","closed_at":"2025-12-21T18:01:04.308Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjf9zd9kgo7","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w3yuv","title":"Drizzle Migration Phase 2: Fix Tests + Hive Conversion","description":"Complete Drizzle migration for swarm-mail package.\n\n## Context\n- Streams subsystem: ✅ Done (wrappers added)\n- Memory subsystem: ✅ Done (already uses Drizzle)\n- Hive subsystem: ❌ Still uses DatabaseAdapter with raw SQL\n\n## Approach\nKeep HiveAdapter interface, rewrite internals to use Drizzle ORM.\nFollow course-builder adapter-drizzle pattern: interface in core, Drizzle implementation under the hood.\n\n## Shared Context for Workers\n- Use `toSwarmDb()` to convert DatabaseAdapter → SwarmDb (Drizzle client)\n- Create wrapper functions matching old signatures for backward compat\n- Keep complex CTEs as raw SQL via `sql.raw()` if Drizzle can't express them\n- Schema source of truth: `packages/swarm-mail/src/db/schema/hive.ts`\n\n## Dependencies\n- Task 1 (fix tests) unblocks CI\n- Task 2 (hive conversion) is the main work\n- Task 3 (schema consolidation) depends on Task 2","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T15:23:32.067Z","updated_at":"2025-12-21T18:01:05.723Z","closed_at":"2025-12-21T18:01:05.723Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w82v6","title":"Fix 6 failing integration tests (CI blocker)","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T15:23:32.072Z","updated_at":"2025-12-21T18:00:54.686Z","closed_at":"2025-12-21T18:00:54.686Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w3yuv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfvn2wanib","title":"Convert hive/store.ts to Drizzle","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:23:32.074Z","updated_at":"2025-12-21T18:00:56.179Z","closed_at":"2025-12-21T18:00:56.179Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w3yuv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfvn2wcdxt","title":"Convert hive/projections.ts to Drizzle","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:23:32.076Z","updated_at":"2025-12-21T18:00:57.636Z","closed_at":"2025-12-21T18:00:57.636Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w3yuv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfvn2webre","title":"Convert hive/queries.ts to Drizzle (or keep raw SQL for complex CTEs)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:23:32.078Z","updated_at":"2025-12-21T18:00:59.516Z","closed_at":"2025-12-21T18:00:59.516Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w3yuv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfvn2wggoh","title":"Convert hive/comments.ts, labels.ts, dependencies.ts to Drizzle","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T15:23:32.080Z","updated_at":"2025-12-21T18:01:01.046Z","closed_at":"2025-12-21T18:01:01.046Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w3yuv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfvn2wiku9","title":"Consolidate duplicate schema definitions","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T15:23:32.082Z","updated_at":"2025-12-21T18:01:02.609Z","closed_at":"2025-12-21T18:01:02.609Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfvn2w3yuv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","title":"Remove PGLite, Port Effect Primitives to libSQL","description":"Complete removal of PGLite infrastructure (except migration tools), port all Effect-TS durable primitives to use libSQL/DatabaseAdapter, and integrate DurableLock + DurableDeferred into swarm worker coordination.\n\n## Context\n- Upstream source: https://github.com/durable-streams/durable-streams\n- Effect primitives: DurableLock, DurableDeferred, DurableCursor, DurableMailbox\n- Current state: primitives use getDatabase() which returns PGLite\n- Target state: primitives accept DatabaseAdapter parameter (libSQL)\n\n## Execution Order\n1. Tasks 0-2 in parallel (Lock, Deferred, Cursor)\n2. Task 3 after 0-2 (Mailbox + ask pattern)\n3. Task 4 after 3 (Remove PGLite from exports)\n4. Tasks 5-6 in parallel after 4 (Integration into swarm)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T16:21:21.697Z","updated_at":"2025-12-21T18:05:01.905Z","closed_at":"2025-12-21T18:05:01.905Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg37mzq","title":"Integrate DurableLock into swarm file reservations","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T16:21:21.715Z","updated_at":"2025-12-21T18:05:03.058Z","closed_at":"2025-12-21T18:05:03.058Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfxpg39jw6","title":"Integrate DurableDeferred into swarm task completion","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T16:21:21.717Z","updated_at":"2025-12-21T18:05:04.159Z","closed_at":"2025-12-21T18:05:04.159Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjfxpg2p165","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg1elo7hfg","title":"Remove PGLite dead code from streams/index.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:04:54.199Z","updated_at":"2025-12-21T18:11:33.570Z","closed_at":"2025-12-21T18:11:33.570Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg1elo0g21","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg1elo0g21","title":"PGLite Cleanup + Effect Primitives Integration","description":"Remove dead PGLite code from streams/index.ts, integrate DurableLock into swarm file reservations, integrate DurableDeferred into swarm task completion. YOLO for v0.31.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T18:04:54.192Z","updated_at":"2025-12-21T18:24:36.778Z","closed_at":"2025-12-21T18:24:36.778Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg1elo9uoa","title":"Integrate DurableLock into swarm file reservations","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:04:54.201Z","updated_at":"2025-12-21T18:24:34.312Z","closed_at":"2025-12-21T18:24:34.312Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg1elo0g21","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg1eloagne","title":"Integrate DurableDeferred into swarm task completion","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:04:54.202Z","updated_at":"2025-12-21T18:24:35.188Z","closed_at":"2025-12-21T18:24:35.188Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg1elo0g21","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg2uw8rcvo","title":"Consolidate per-project DBs to centralized global DB","description":"Currently each project gets its own `.opencode/streams.db` database, with fallback to `~/.opencode/streams.db`. This causes:\n\n1. **Schema drift** - Different projects have different schema versions, causing \"no such column: stream\" errors\n2. **Migration complexity** - Need to migrate each project's DB separately\n3. **Disk bloat** - Multiple copies of similar data\n4. **Cross-project queries impossible** - Can't search across all projects\n\n**Proposed solution:**\n- Single global `~/.opencode/swarm-mail.db` for all projects\n- Project isolation via `project_key` column (already exists in schema)\n- Single migration path\n- Optional per-project override for special cases\n\n**Migration path:**\n1. Add migration tool to merge existing per-project DBs into global\n2. Update `getSwarmMailLibSQL()` to prefer global DB\n3. Deprecate per-project DB creation\n4. Clean up old per-project DBs after successful migration\n\n**Related issues:**\n- \"SQLITE_ERROR: no such column: stream\" errors in tests and other sessions\n- PGLite → libSQL migration incomplete in some projects","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-21T18:45:34.011Z","updated_at":"2025-12-21T19:10:18.780Z","closed_at":"2025-12-21T19:10:18.780Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg37qv7vim","title":"Auto-migration to centralized swarm tools database","description":"On swarmmail_init, detect old per-project DBs (.opencode/streams.db or .opencode/streams/ PGLite), migrate data to global ~/.opencode/swarm-mail.db, rename old DB to .backup with timestamp. Fast check on init, handles both libSQL and PGLite sources.\n\n## Design Decisions\n- Trigger: On swarmmail_init (natural entry point, once per session)\n- Old DB handling: Rename to .backup-<timestamp> (safe rollback)\n- Global DB: ~/.opencode/swarm-mail.db\n- Project isolation: via project_key column (already exists)\n\n## Lore (from DDIA)\n\"In most cases, a change to an application's features also requires a change to data that it stores\" - handle schema evolution gracefully, missing tables are OK.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T18:55:33.572Z","updated_at":"2025-12-21T19:10:16.910Z","closed_at":"2025-12-21T19:10:16.910Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg37qvlyi9","title":"Create complete auto-migration module","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T18:55:33.585Z","updated_at":"2025-12-21T19:09:14.664Z","closed_at":"2025-12-21T19:09:14.664Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg37qv7vim","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg37qvom6o","title":"Update getDatabasePath to use global DB","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T18:55:33.588Z","updated_at":"2025-12-21T19:09:16.029Z","closed_at":"2025-12-21T19:09:16.029Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg37qv7vim","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg37qvrqhw","title":"Integrate auto-migration into swarmmail_init","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:55:33.591Z","updated_at":"2025-12-21T19:09:17.831Z","closed_at":"2025-12-21T19:09:17.831Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg37qv7vim","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg37qw04am","title":"Add tests for auto-migration","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:55:33.600Z","updated_at":"2025-12-21T19:09:19.036Z","closed_at":"2025-12-21T19:09:19.036Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg37qv7vim","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg37qw2oq7","title":"Update exports and documentation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:55:33.602Z","updated_at":"2025-12-21T19:10:14.566Z","closed_at":"2025-12-21T19:10:14.566Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg37qv7vim","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg3injmw1p","title":"Fix global DB path: ~/.config/swarm-tools/swarm.db","description":"User requested path change after initial implementation.\n\nChange from: ~/.opencode/swarm-mail.db\nChange to: ~/.config/swarm-tools/swarm.db\n\nFiles to update:\n1. packages/swarm-mail/src/streams/auto-migrate.ts - getGlobalDbPath()\n2. packages/swarm-mail/src/streams/index.ts - getDatabasePath()\n3. Any tests that reference the old path\n\nRationale: Cleaner separation from OpenCode config, follows XDG conventions.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T19:04:02.482Z","updated_at":"2025-12-21T19:09:13.065Z","closed_at":"2025-12-21T19:09:13.065Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg37qv7vim","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg23zk1qi7","title":"Remove UBS scan from swarm_complete","description":"Remove the UBS scan step from swarm_complete. It's slowing things down and not providing value for the release.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:24:38.593Z","updated_at":"2025-12-21T19:44:05.642Z","closed_at":"2025-12-21T19:44:05.642Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg59pcddrx","title":"Holistic Docs Refresh","description":"Full documentation overhaul with practical focus. OpenCode users should be running /swarm in under 1 minute.\n\nGoals:\n- Keep the \"why\" (problem statement)\n- Accurate tech stack (libSQL not PGLite, no UBS scan)\n- Terminology: beads→hive/cells throughout\n- Sick ASCII art (bee/hive/swarm theme)\n- Quotes from pdf-brain sprinkled in\n- Diagrams where useful\n- No marketing fluff, just explanation\n- Use cases front and center\n\nQuote seeds from pdf-brain:\n- \"With event sourcing, you can design an event such that it is a self-contained description of a user action.\" — Kleppmann, DDIA\n- \"ROC accepts that failures will inevitably happen... investigations aim to improve survivability\" — Release It!\n- \"Architecture boils down to the important stuff—whatever that is.\" — Fowler, PEAA\n- \"High-variability sequencing of whole-task problems\" — 4C/ID Model","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T19:53:04.141Z","updated_at":"2025-12-21T19:59:12.464Z","closed_at":"2025-12-21T19:59:12.464Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg59pcltt5","title":"Homepage rewrite","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T19:53:04.149Z","updated_at":"2025-12-21T19:59:06.124Z","closed_at":"2025-12-21T19:59:06.124Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg59pcddrx","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg59pcnvlc","title":"Root README update","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T19:53:04.151Z","updated_at":"2025-12-21T19:59:07.923Z","closed_at":"2025-12-21T19:59:07.923Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg59pcddrx","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg59pcpwx4","title":"Plugin README update","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T19:53:04.153Z","updated_at":"2025-12-21T19:59:09.181Z","closed_at":"2025-12-21T19:59:09.181Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg59pcddrx","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg59pcsmey","title":"Web docs terminology sweep","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T19:53:04.156Z","updated_at":"2025-12-21T19:59:10.895Z","closed_at":"2025-12-21T19:59:10.895Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg59pcddrx","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg5hwga30a","title":"Evaluate CASS for inhousing - nuke the dep","description":"Deep dive into https://github.com/Dicklesworthstone/coding_agent_session_search\n\n**Goal:** Determine if we should bring CASS functionality in-house and eliminate the external Python dependency.\n\n**Questions to answer:**\n1. What agent session formats does it index? (Claude, Cursor, Codex, etc.)\n2. How does it build the search index? (embedding model, chunking strategy)\n3. What's the query interface? (semantic search, filters, ranking)\n4. How much of this overlaps with our semantic memory?\n5. What's unique/valuable that we'd lose without it?\n\n**If we inhouse:**\n- Scan agent session files ourselves (add to libSQL index)\n- Unify search: one `memory_find()` for both curated learnings AND historical sessions\n- Cross-agent support built into swarm-mail\n- No Python dep, no separate `cass index` command\n\n**Deliverables:**\n1. Architecture analysis doc\n2. Recommendation: inhouse vs keep dep vs hybrid\n3. If inhouse: implementation plan with effort estimate\n\n**Related:** We already have semantic-memory with libSQL + Ollama embeddings. CASS uses its own embedding pipeline. Could we unify?","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-21T19:59:26.602Z","updated_at":"2025-12-21T19:59:26.602Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg5owr6vxb","title":"Fix DB adapter wiring + add integration test coverage","description":"Eliminate all \"dbOverride parameter is required\" errors by fixing code that calls swarmMail.getDatabase() expecting raw query access. The adapter pattern requires passing DatabaseAdapter explicitly to store functions. Also add integration tests to catch these issues in the future.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-21T20:04:53.586Z","updated_at":"2025-12-21T21:02:57.661Z","closed_at":"2025-12-21T21:02:57.661Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg5owrdvkk","title":"Fix swarm-mail internals - remove getDatabase() requirement from store/projections","description":"REDO with proper TDD. Tests FIRST, then implementation. Previous attempt skipped to implementation.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T20:04:53.593Z","updated_at":"2025-12-21T20:21:55.877Z","closed_at":"2025-12-21T20:21:55.877Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg5owr6vxb","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg5owrf7bf","title":"Fix plugin runtime - update swarm-orchestrate.ts, hive.ts, memory-tools.ts, swarm-mail.ts","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T20:04:53.595Z","updated_at":"2025-12-21T20:32:30.032Z","closed_at":"2025-12-21T20:32:30.032Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg5owr6vxb","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg5owrhr3v","title":"Update test files using old getDatabase() pattern","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T20:04:53.597Z","updated_at":"2025-12-21T20:46:42.634Z","closed_at":"2025-12-21T20:46:42.634Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg5owr6vxb","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg5owrjo4f","title":"Add tool-layer integration tests that verify adapter wiring end-to-end","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T20:04:53.599Z","updated_at":"2025-12-21T20:54:41.568Z","closed_at":"2025-12-21T20:54:41.568Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg5owr6vxb","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjg6oywchz2","title":"Add mandatory coordinator review loop after worker completion","description":"The coordinator prompt in AGENTS.md mentions swarm_review but doesn't COMMAND it after every worker return.\n\nNeed to:\n1. Update swarm-prompts.ts to add a COORDINATOR_REVIEW_LOOP constant\n2. Update swarm_spawn_subtask to include post-completion instructions for coordinator\n3. Update bin/swarm.ts coordinator prompt to MANDATE the review loop\n4. Update global-skills/swarm-coordination/SKILL.md with explicit review requirements\n\nThe pattern should be:\n```\nTask(worker) returns → IMMEDIATELY:\n1. swarmmail_inbox() - check for worker messages\n2. swarm_review(project_key, epic_id, task_id, files_touched)\n3. Review the diff against epic goals\n4. swarm_review_feedback(status=\"approved|needs_changes\")\n5. ONLY THEN spawn next worker or complete\n```","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-21T20:32:55.980Z","updated_at":"2025-12-21T21:02:51.097Z","closed_at":"2025-12-21T21:02:51.097Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjg5owr6vxb","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgh4b3kmi1","title":"Fix PGLite typecheck errors in CI","description":"CI failing: Cannot find module '@electric-sql/pglite'. Migration files have static imports but PGLite is devDependency.\n\nFix: Convert static imports to dynamic imports in:\n- src/memory/migrate-legacy.ts (lines 146-147, 305-306)\n- src/migrate-pglite-to-libsql.ts (lines 93-94)\n- src/pglite.ts (line 10, plus fix implicit any on line 35)\n\nUse pattern from semantic memory: dynamic import inside functions, type assertions for results.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-22T01:24:47.792Z","updated_at":"2025-12-22T01:27:32.279Z","closed_at":"2025-12-22T01:27:32.279Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgha1ijy85","title":"Add @ts-ignore to PGLite dynamic imports","description":"Dynamic imports still get type-checked by TypeScript. Need to add @ts-ignore comments above each dynamic import line.\n\nFiles:\n- src/memory/migrate-legacy.ts (lines 146, 147, 305, 306)\n- src/migrate-pglite-to-libsql.ts (lines 93, 94)\n\nPattern:\n```typescript\n// @ts-ignore - PGLite is optional, loaded dynamically for migration only\nconst { PGlite } = (await import(\"@electric-sql/pglite\")) as any;\n// @ts-ignore - PGLite vector extension\nconst { vector } = (await import(\"@electric-sql/pglite/vector\")) as any;\n```","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-22T01:29:15.307Z","updated_at":"2025-12-22T01:30:59.948Z","closed_at":"2025-12-22T01:30:59.948Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjghm2c8rkt","title":"Fix migrate-legacy.test.ts PGLite import","description":"Test file still has static PGLite import:\n```\nerror: Cannot find module '@electric-sql/pglite' from '/home/runner/work/swarm-tools/swarm-tools/packages/swarm-mail/src/memory/migrate-legacy.test.ts'\n```\n\nNeed to either:\n1. Skip the test when PGLite isn't available\n2. Use dynamic import in the test file too\n3. Mark the test as integration-only","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-22T01:38:36.248Z","updated_at":"2025-12-22T01:43:02.311Z","closed_at":"2025-12-22T01:43:02.311Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjghm3oyxgp","title":"Fix cell ID prefix test - package.json name not found","description":"Test failing:\n```\nExpected substring or pattern: /^swarm-mail-[-a-z0-9]+-[a-z0-9]+$/\nReceived: \"cell--d02u8-mjghd0qj2bg\"\n```\n\nThe generateBeadId function isn't finding the package.json name in CI. The prefix is empty, resulting in `cell--` instead of `swarm-mail-`.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-22T01:38:38.002Z","updated_at":"2025-12-22T01:43:03.048Z","closed_at":"2025-12-22T01:43:03.048Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgi42mfkmv","title":"Fix agent-mail.test.ts - checkHealth and DurableLock tests using deprecated API","description":"Tests in agent-mail.test.ts are calling deprecated `checkHealth()` which now throws:\n```\n[agent-mail] checkHealth() has been removed. PGlite infrastructure is deprecated.\n```\n\nAffected tests:\n- checkHealth > returns healthy when database is accessible\n- checkHealth > returns stats about the store\n- DurableLock Integration > uses DurableLock for file reservation locking\n- DurableLock Integration > releases DurableLock when files are released\n- DurableLock Integration > respects lock expiry (TTL)\n- reserveAgentFiles tests (multiple)\n- releaseAgentFiles tests (multiple)\n\nOptions:\n1. Update tests to use createSwarmMailAdapter() pattern\n2. Skip these tests with a TODO to update later\n3. Restore checkHealth with adapter-based implementation\n\nGiven time pressure, option 2 (skip with TODO) is fastest.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-22T01:52:36.423Z","updated_at":"2025-12-22T01:56:25.828Z","closed_at":"2025-12-22T01:56:25.828Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgicv66eyc","title":"Fix missing resetDatabase/closeDatabase exports in test files","description":"Two test files import functions that were removed:\n- debug.test.ts imports resetDatabase\n- projections.test.ts imports closeDatabase\n\nThese functions were removed with PGLite. Need to either:\n1. Skip these tests\n2. Update to use adapter pattern","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-22T01:59:26.670Z","updated_at":"2025-12-22T02:05:24.529Z","closed_at":"2025-12-22T02:05:24.529Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmdsjknsx","title":"Fix SQL injection in cursor.ts - use parameterized queries","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-22T03:52:08.385Z","updated_at":"2025-12-22T03:52:08.385Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmdsjc88p","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmdsjosz1","title":"Fix SQL injection in lock.ts - use parameterized queries","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-22T03:52:08.388Z","updated_at":"2025-12-22T03:52:08.388Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmdsjc88p","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmw8ad8da","title":"Fix swarm-mail.test.ts to use adapter pattern","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-22T04:06:28.597Z","updated_at":"2025-12-22T04:06:28.597Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmw89cjom","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmw8ah72p","title":"Clean test artifacts and commit all fixes","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-22T04:06:28.601Z","updated_at":"2025-12-22T04:06:28.601Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmw89cjom","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgotvvoh8z","title":"Fix DatabaseAdapter import in agent-mail.ts (build broken)","description":"**CRITICAL: Build is broken**\n\nThe worker's changes to agent-mail.ts imported DatabaseAdapter from the wrong path:\n```typescript\nimport type { DatabaseAdapter } from \"../adapter\";  // WRONG\n```\n\nShould be:\n```typescript\nimport type { DatabaseAdapter } from \"../types/database\";  // CORRECT\n```\n\nError:\n```\nsrc/streams/agent-mail.ts(24,15): error TS2459: Module '\"../adapter\"' declares 'DatabaseAdapter' locally, but it is not exported.\n```\n\nThis blocks the entire release - typecheck and build both fail.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-22T05:00:38.436Z","updated_at":"2025-12-22T05:01:11.424Z","closed_at":"2025-12-22T05:01:11.424Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgqqmtgx08","title":"swarmmail_health uses deprecated checkHealth() that was removed","description":"**Problem:** `swarmmail_health` tool calls the deprecated `checkHealth()` function that was removed during PGlite cleanup.\n\n**Error:**\n```\nHealth check failed: [agent-mail] checkHealth() has been removed. PGlite infrastructure is deprecated. Use createSwarmMailAdapter() and call adapter.checkHealth() instead.\n```\n\n**Expected:** Health check should use the new adapter pattern, not the removed function.\n\n**Fix:** Update swarmmail_health in swarm-mail.ts to use `createSwarmMailAdapter().checkHealth()` or equivalent libSQL health check.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-22T05:54:05.956Z","updated_at":"2025-12-22T06:13:21.000Z","closed_at":"2025-12-22T06:13:21.000Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgqqp1izbr","title":"swarmmail_inbox fails with \"no such table: messages\" - schema not initialized","description":"**Problem:** `swarmmail_inbox` fails because the messages table doesn't exist.\n\n**Error:**\n```\nFailed to fetch inbox: SQLITE_ERROR: no such table: messages\n```\n\n**Expected:** Schema should be auto-initialized when swarmmail tools are used, or `swarmmail_init` should ensure schema exists.\n\n**Root cause:** The libSQL schema initialization isn't being called before querying. Either:\n1. `swarmmail_init` doesn't create the schema\n2. `swarmmail_inbox` doesn't ensure schema exists before querying\n\n**Fix:** Ensure `createLibSQLStreamsSchema()` is called during init or before first query.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-22T05:54:08.838Z","updated_at":"2025-12-22T06:13:22.317Z","closed_at":"2025-12-22T06:13:22.317Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmdsjc88p","title":"Fix PR #54 review comments","description":"Address CodeRabbit review comments:\n1. SQL injection vulnerabilities in cursor.ts and lock.ts\n2. Test pollution in .hive/memories.jsonl","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T03:52:08.376Z","updated_at":"2025-12-22T16:45:20.225Z","closed_at":"2025-12-22T16:45:20.225Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmdsjq0gb","title":"Remove test artifacts from .hive/memories.jsonl","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T03:52:08.390Z","updated_at":"2025-12-22T16:45:25.538Z","closed_at":"2025-12-22T16:45:25.538Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmdsjc88p","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmw89cjom","title":"PR #54: Fix skipped tests and resolve CodeRabbit comments","description":"Delete deprecated test files, fix/unskip remaining tests, commit SQL injection fixes, fix double file: prefix bug, clean test artifacts, push and resolve PR comments","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T04:06:28.560Z","updated_at":"2025-12-22T16:45:21.082Z","closed_at":"2025-12-22T16:45:21.082Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmw89p9re","title":"Delete deprecated debug module and tests","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T04:06:28.573Z","updated_at":"2025-12-22T16:45:26.311Z","closed_at":"2025-12-22T16:45:26.311Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmw89cjom","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmw89uekd","title":"Delete deprecated test files (migrations.test.ts, projections.test.ts)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T04:06:28.578Z","updated_at":"2025-12-22T16:45:27.218Z","closed_at":"2025-12-22T16:45:27.218Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmw89cjom","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmw8a3wqf","title":"Fix double file: prefix in store-drizzle.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T04:06:28.587Z","updated_at":"2025-12-22T16:45:28.218Z","closed_at":"2025-12-22T16:45:28.218Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmw89cjom","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgmw8a89uc","title":"Fix agent-mail.test.ts skipped reservation tests","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T04:06:28.592Z","updated_at":"2025-12-22T16:45:28.958Z","closed_at":"2025-12-22T16:45:28.958Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjgmw89cjom","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhdvwcbb5f","title":"Clean test artifacts from .hive/","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T16:42:02.747Z","updated_at":"2025-12-22T16:44:06.368Z","closed_at":"2025-12-22T16:44:06.368Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhdvwbu2iv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhdvwbu2iv","title":"PR #54 Final Cleanup","description":"Clean up test artifacts, fix test isolation, verify URL normalization fix, close orphan cells.\n\nContext from semantic memory:\n- sendSwarmMessage was failing with URL_INVALID for bare paths - FIXED by URL normalization in createLibSQLAdapter\n- Test isolation issue: tests writing to prod .hive/ instead of temp directories\n- 3 \"E2E Test Epic\" artifacts + 6 subtasks need cleanup from issues.jsonl\n- 1 test memory needs removal from memories.jsonl","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T16:42:02.730Z","updated_at":"2025-12-22T16:53:23.084Z","closed_at":"2025-12-22T16:53:23.084Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhdvwcftl0","title":"Fix test isolation - prevent tests writing to prod .hive/","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T16:42:02.752Z","updated_at":"2025-12-22T16:53:15.728Z","closed_at":"2025-12-22T16:53:15.728Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhdvwbu2iv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhdvwcigod","title":"Verify sendSwarmMessage URL fix and unskip tests","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T16:42:02.754Z","updated_at":"2025-12-22T16:53:16.991Z","closed_at":"2025-12-22T16:53:16.991Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhdvwbu2iv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhdvwckiyk","title":"Close duplicate/orphan cells and sync","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T16:42:02.756Z","updated_at":"2025-12-22T16:53:17.768Z","closed_at":"2025-12-22T16:53:17.768Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhdvwbu2iv","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhef7g97ss","title":"Fix getInbox returns empty in swarm-review integration tests","description":"Bug: swarm-review.integration.test.ts tests pass sendSwarmMessage but getInbox returns empty.\n\nHypothesis from semantic memory:\n- sendSwarmMessage creates its own LibSQLAdapter\n- Test uses a different in-memory adapter instance\n- Messages written to one DB, read from another = empty\n\nTDD approach:\n1. Write/unskip failing test that reproduces the bug\n2. Trace the adapter creation path\n3. Fix the adapter sharing issue\n4. Verify test passes","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T16:57:03.609Z","updated_at":"2025-12-22T17:10:48.958Z","closed_at":"2025-12-22T17:10:48.958Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhef7gh5cy","title":"Investigate and fix getInbox empty bug with TDD","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-22T16:57:03.617Z","updated_at":"2025-12-22T17:10:48.320Z","closed_at":"2025-12-22T17:10:48.320Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhef7g97ss","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhfac87zak","title":"Resolve PR #54 Comments + Create pr-triage Skill","description":"Hybrid approach: fix code issues with TDD, reply won't-fix to minor/style issues, create reusable pr-triage skill for future PR reviews. All 5 subtasks can run in parallel.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T17:21:16.135Z","updated_at":"2025-12-22T17:30:53.519Z","closed_at":"2025-12-22T17:30:53.519Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhfac8da6s","title":"Create pr-triage skill with context-efficient patterns","description":"Create a new skill at .opencode/skills/pr-triage/ (PROJECT-LEVEL, not global) that teaches agents how to efficiently triage and respond to PR comments.\n\nKEY REQUIREMENTS:\n1. SKILL.md must include:\n   - Context-efficient workflow: fetch metadata first (id, path, line, severity), only fetch full body when needed\n   - gh API patterns for listing comments without blowing context\n   - Triage categories: fix-with-code, won't-fix, tracked-in-cell\n   - Response templates for each category\n   - CodeRabbit-specific patterns (severity markers, proposed fixes)\n\n2. references/gh-api-patterns.md with:\n   - Compact jq queries for comment metadata\n   - Thread detection (in_reply_to_id)\n   - Severity extraction from CodeRabbit format\n   - Reply posting via gh api\n\nUse skills_create tool with scope=\"project\" to create the skill, then add content.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T17:21:16.141Z","updated_at":"2025-12-22T17:28:57.715Z","closed_at":"2025-12-22T17:28:57.715Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhfac87zak","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhfac8fxvn","title":"Reply to .hive/issues.jsonl comments (already cleaned)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T17:21:16.143Z","updated_at":"2025-12-22T17:28:59.215Z","closed_at":"2025-12-22T17:28:59.215Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhfac87zak","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhfac8ielf","title":"Fix adapter.test.ts - add closed_reason verification (TDD)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T17:21:16.146Z","updated_at":"2025-12-22T17:29:00.141Z","closed_at":"2025-12-22T17:29:00.141Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhfac87zak","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhfac8kn2t","title":"Fix agent-mail.ts DurableLock error handling (TDD)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T17:21:16.148Z","updated_at":"2025-12-22T17:29:01.286Z","closed_at":"2025-12-22T17:29:01.286Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhfac87zak","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhfac8lnmh","title":"Reply won't-fix to minor/style comments","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-22T17:21:16.149Z","updated_at":"2025-12-22T17:29:02.301Z","closed_at":"2025-12-22T17:29:02.301Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhfac87zak","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhgw0g8l1v","title":"Holistic Docs Update - Human-Forward, Happy Path First","description":"Full docs refresh post-v0.32 (Drizzle migration). Focus on human-forward content with happy path front and center. Update all READMEs and docs site to reflect libSQL storage, 95% test coverage, coordinator review gate, and other v0.31-0.32 changes.","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-22T18:06:06.920Z","updated_at":"2025-12-22T18:06:06.920Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhgw0gqqo8","title":"Create CHANGELOG.md from changesets","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-22T18:06:06.938Z","updated_at":"2025-12-22T18:06:06.938Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhgw0g8l1v","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhji3rxvs0","title":"ADR: Runtime Visibility - Live Dashboards & Status","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:19:16.893Z","updated_at":"2025-12-22T19:24:49.977Z","closed_at":"2025-12-22T19:24:49.977Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhji3rrl06","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5nomh","title":"Add deprecation warnings to skills_list, skills_use, skills_read, skills_execute","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:25:18.971Z","updated_at":"2025-12-22T19:29:04.031Z","closed_at":"2025-12-22T19:29:04.031Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5hxwu","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5qo58","title":"Audit bundled skills for Agent Skills spec compliance","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:25:18.974Z","updated_at":"2025-12-22T19:29:05.347Z","closed_at":"2025-12-22T19:29:05.347Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5hxwu","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5rt60","title":"Update tests for deprecated tools","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T19:25:18.975Z","updated_at":"2025-12-22T19:35:17.543Z","closed_at":"2025-12-22T19:35:17.543Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5hxwu","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhk4kkh975","title":"Implement Observability Stack (from ADRs)","description":"## Observability Implementation Plan\n\nBased on research spike ADRs:\n- `.hive/analysis/observability-runtime-visibility.md`\n- `.hive/analysis/observability-post-hoc-analysis.md`\n- `.hive/analysis/observability-developer-debugging.md`\n\n### Key Insight\nEvent sourcing is 80% of the solution. We have 17+ event types, distributed tracing built-in (epic_id = trace_id, bead_id = span_id), and libSQL for queries. We need **diagnostic views**, not new infrastructure.\n\n---\n\n## Proposed Phases\n\n### Phase 1: Error Enrichment + Verbose Logging (3 days)\n- [ ] Structured error context (file, line, agent, epic, recent events)\n- [ ] DEBUG=swarm:* environment variable filtering\n- [ ] Error suggestions based on common patterns\n- **Value:** Immediate debugging improvement, low effort\n\n### Phase 2: SQL CLI for Analytics (1 week)\n- [ ] `swarm query` CLI command with SQL interface\n- [ ] 10 pre-built queries (failed decompositions, duration by strategy, file conflicts)\n- [ ] Output formats: table, JSON, CSV\n- **Value:** \"Why did this fail?\" answerable without raw SQL\n\n### Phase 3: Terminal UI Dashboard (1-2 weeks)\n- [ ] blessed-contrib live dashboard\n- [ ] Worker status, progress bars, file locks, recent messages\n- [ ] Multi-swarm view (tabs or split)\n- **Value:** Real-time visibility without polling tools manually\n\n### Phase 4: Event Replay CLI (1 day)\n- [ ] `swarm replay <epic-id>` command\n- [ ] Replay events to stdout with timing\n- [ ] Filter by event type, agent, time range\n- **Value:** Reproduce failures locally (within LLM non-determinism)\n\n### Phase 5: Export Formats (1 week)\n- [ ] OpenTelemetry trace export\n- [ ] CSV export for spreadsheet analysis\n- [ ] JSON export for external tools\n- **Value:** Integration with existing observability stacks\n\n### Phase 6: Web Dashboard (3-4 weeks) - DEFERRED\n- [ ] React/Next.js dashboard\n- [ ] Timeline visualization\n- [ ] Dependency graphs\n- **Value:** Rich visualization, multi-user access\n- **Defer until:** Multi-swarm scenarios proven in production\n\n---\n\n## Success Criteria\n- [ ] Can answer \"why did epic X fail?\" in <2 minutes\n- [ ] Can see swarm progress in real-time without polling\n- [ ] Can export traces to external tools\n- [ ] No context bloat from verbose logging\n\n## References\n- ADR: Runtime Visibility → Terminal UI + structured logs\n- ADR: Post-Hoc Analysis → SQL CLI → exports → visualization  \n- ADR: Developer Debugging → replay + state dumps + error enrichment","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-22T19:36:45.089Z","updated_at":"2025-12-22T19:36:45.089Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5hxwu","title":"Deprecate skills system for OpenCode native compatibility","description":"Soft deprecate discovery/loading tools (skills_list, skills_use, skills_read, skills_execute) with warnings. Keep authoring tools. Ensure bundled skills are spec-compliant.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T19:25:18.965Z","updated_at":"2025-12-22T19:37:53.190Z","closed_at":"2025-12-22T19:37:53.190Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5t7z5","title":"Create changeset for deprecation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T19:25:18.977Z","updated_at":"2025-12-22T19:37:44.912Z","closed_at":"2025-12-22T19:37:44.912Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhjpv5hxwu","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkiumsuk1","title":"swarm-db CLI Entry Point and Commands","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-22T19:47:51.316Z","updated_at":"2025-12-22T19:47:51.316Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhkiuletid","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkiulp1uv","title":"Structured Error Classes with Context Enrichment","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:47:51.277Z","updated_at":"2025-12-22T19:58:39.893Z","closed_at":"2025-12-22T19:58:39.893Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhkiuletid","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkiulwzvh","title":"Verbose Logging with DEBUG Environment Variable","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:47:51.284Z","updated_at":"2025-12-22T19:58:40.796Z","closed_at":"2025-12-22T19:58:40.796Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhkiuletid","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkium1nmk","title":"Analytics Query Builder and Types","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:47:51.289Z","updated_at":"2025-12-22T19:58:41.641Z","closed_at":"2025-12-22T19:58:41.641Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhkiuletid","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkium6rpy","title":"Pre-built Analytics Queries (1-5)","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-12-22T19:47:51.294Z","updated_at":"2025-12-22T20:03:08.359Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhkiuletid","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkiumaibf","title":"Pre-built Analytics Queries (6-10)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T19:47:51.298Z","updated_at":"2025-12-22T20:07:48.342Z","closed_at":"2025-12-22T20:07:48.342Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhkiuletid","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkvabovqo","title":"Agent-facing Analytics Tools (MCP/Plugin Integration)","description":"Expose observability tools to agents via plugin tools, not just CLI.\n\n## Why This Matters\nAgents need to:\n1. **Debug swarm failures** - query why an epic failed without human intervention\n2. **Generate insights** - \"what strategies are failing?\" for learning loops\n3. **Self-diagnose** - when blocked, query recent events for context\n4. **Report to coordinator** - structured analytics in swarm mail\n\n## Tools to Add\n\n### `swarm_analytics` - Query pre-built analytics\n```typescript\nswarm_analytics({\n  query: \"failed-decompositions\" | \"strategy-success-rates\" | \"lock-contention\" | ...,\n  since?: string,  // \"7d\", \"24h\"\n  format?: \"json\" | \"summary\"  // summary = human-readable for context\n})\n```\n\n### `swarm_query` - Raw SQL for power users\n```typescript\nswarm_query({\n  sql: \"SELECT * FROM events WHERE type = 'task_blocked' LIMIT 5\",\n  format?: \"json\" | \"table\"\n})\n```\n\n### `swarm_diagnose` - Auto-diagnosis for current epic\n```typescript\nswarm_diagnose({\n  epic_id: string,\n  include?: [\"blockers\", \"conflicts\", \"slow_tasks\", \"errors\"]\n})\n// Returns structured diagnosis with suggestions\n```\n\n### `swarm_insights` - Generate learning insights\n```typescript\nswarm_insights({\n  scope: \"epic\" | \"project\" | \"all\",\n  epic_id?: string,\n  metrics: [\"success_rate\", \"duration\", \"conflicts\", \"retries\"]\n})\n// Returns insights suitable for semantic-memory storage\n```\n\n## Integration Points\n- Add to `packages/opencode-swarm-plugin/src/plugin.ts`\n- Use analytics module from swarm-mail\n- Context-safe output (truncate large results)\n- JSON output for agent consumption\n\n## TDD Requirements\n- Test each tool returns valid structured output\n- Test context limits (max rows, truncation)\n- Test error handling (invalid queries, missing epic)\n\n## Files\n- packages/opencode-swarm-plugin/src/observability-tools.ts (new)\n- packages/opencode-swarm-plugin/src/observability-tools.test.ts (new)\n- packages/opencode-swarm-plugin/src/plugin.ts (add tools)","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-22T19:57:31.524Z","updated_at":"2025-12-22T19:57:31.524Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhkiuletid","title":"Observability Stack MVP: Error Enrichment + SQL CLI","description":"TDD-driven implementation of Phase 1 (structured error classes with context/suggestions, DEBUG env var filtering) and Phase 2 (swarm-db CLI with raw SQL and 10 pre-built analytics queries). Every feature starts with a failing test.\n\n## Knowledge Sources\n- Four Golden Signals (SRE): latency, traffic, errors, saturation → mapped to analytics queries\n- Agent Observability: agent-aware tracing with bead_id/epic_id correlation\n- Event Sourcing: SQL CLI as diagnostic view over existing 17 event types\n- TDD Lore: Kent Beck, Sandi Metz - red-green-refactor, characterization tests\n\n## Success Criteria\n- All error classes have toJSON() with rich context + suggestions\n- DEBUG=swarm:* filters log output by subsystem\n- `swarm-db query` executes raw SQL against libSQL\n- `swarm-db analytics` runs 10 pre-built queries with --format, --since, --until\n- 100% test coverage on new code (unit + integration)\n\n## Parent Epic\nopencode-swarm-monorepo-lf2p4u-mjhk4kkh975","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T19:47:51.266Z","updated_at":"2025-12-22T20:24:42.998Z","closed_at":"2025-12-22T20:24:42.998Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfyo44apwt","title":"Add Haiku-Powered Compaction Analysis","description":"Wire liteModel into plugin template and use it for pre-analysis during compaction.\n\n1. Parameterize getPluginWrapper() to accept liteModel\n2. Update compaction hook to call `opencode run -m <liteModel>` for pre-analysis\n3. Use output.prompt to replace compaction prompt with pre-analyzed state\n\nThis enables smarter compaction that preserves swarm state for continuation.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-21T16:48:19.162Z","updated_at":"2025-12-23T16:46:43.839Z","closed_at":"2025-12-23T16:46:43.839Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjithnuayc9","title":"LLM-Powered Compaction with output.prompt Replacement","description":"Leverage the new `output.prompt` API from OpenCode PR #5907 to completely replace the compaction prompt with an LLM-generated swarm-forward continuation prompt.\n\n## Key Features\n1. Use lite model (Haiku or user-configured) via `opencode run -m <liteModel>`\n2. Generate dynamic continuation prompt based on actual swarm state\n3. Progressive enhancement - gracefully degrade on older OpenCode versions\n4. Fallback to static context if LLM call fails\n\n## Design Decisions\n- Shell out to `opencode run` (no new dependencies)\n- Guard against missing `output.prompt` API\n- Fallback chain: LLM prompt → static context → detection fallback\n\n## Related\n- OpenCode PR: https://github.com/sst/opencode/pull/5907\n- Existing cell: opencode-swarm-monorepo-lf2p4u-mjfyo44apwt","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-23T16:46:38.578Z","updated_at":"2025-12-23T16:54:33.156Z","closed_at":"2025-12-23T16:54:33.156Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjithnuh0pv","title":"ADR: LLM-Powered Compaction Architecture","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T16:46:38.585Z","updated_at":"2025-12-23T16:50:34.232Z","closed_at":"2025-12-23T16:50:34.232Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjithnuayc9","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjithnujolm","title":"Implement LLM Compaction in plugin-wrapper-template.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T16:46:38.587Z","updated_at":"2025-12-23T16:54:31.512Z","closed_at":"2025-12-23T16:54:31.512Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjithnuayc9","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfzlbckh37","title":"Create Hive/Cell Visualizer","description":"Build a visualizer for hive cells inspired by https://github.com/Dicklesworthstone/beads_viewer and https://github.com/mantoni/beads-ui\n\n**ADR:** `.hive/analysis/hive-cell-visualizer.md`\n\n## Decision Summary (Updated 2025-12-23)\n\nBuild a **three-part visualizer** using modern stack:\n\n1. **CLI Query Tool** (`swarm viz`): Quick terminal-based status views\n2. **TanStack Start Web App** (`swarm viz --serve`): Real-time interactive visualization\n3. **Static HTML Export** (`swarm viz --export`): Self-contained snapshot for sharing\n\n## Key Architecture Decisions\n\n### TanStack Start + Durable Streams\n\n- **TanStack Start**: Type-safe routing, SSR + streaming, server functions, Vite-powered\n- **Durable Streams Protocol**: Electric's open protocol for real-time sync\n  - Offset-based resumability (refresh-safe, multi-device, multi-tab)\n  - Long-poll and SSE modes for live tailing\n  - CDN-friendly design for massive fan-out\n\n### Why Durable Streams over WebSocket?\n\n1. **Refresh-safe**: User refreshes → picks up exactly where they left off\n2. **Multi-tab**: Multiple tabs share stream without duplicating connections\n3. **Production-proven**: 1.5 years at Electric, millions of events/day\n\n## Implementation Phases\n\n1. **Phase 1**: Durable Streams adapter for swarm-mail events\n2. **Phase 2**: Data layer + CLI views (status table, tree, kanban)\n3. **Phase 3**: TanStack Start web app with live updates\n4. **Phase 4**: Static HTML export + plugin integration\n\n## Technical Stack\n\n- TypeScript/Bun\n- TanStack Start (React framework)\n- @durable-streams/client for browser subscription\n- force-graph for visualization\n- libSQL event store (existing swarm-mail)\n\n---\n\n## 🔗 Related: Logging Infrastructure\n\n**Cell:** `opencode-swarm-plugin--ys7z8-mjk6pwwn9nw`\n\nPino-based logging (`~/.config/swarm-tools/logs/`) feeds into the visualizer:\n- `swarm log` CLI for querying structured logs\n- Compaction hook as first instrumentation target\n- Logs panel in web UI showing real-time activity\n\n---\n\n## 🔮 Future Vision: Control Plane\n\n**Memory:** `cdcf917a-f473-4d2d-9bb5-63baa35baa3b`\n\nThe viewer evolves from read-only dashboard to **control plane**:\n\n1. **Dynamic Prompts** - Edit coordinator/worker prompts in UI, stored in libSQL\n2. **Compaction Tuning** - Adjust context priorities, token budgets, preservation rules\n3. **Skills Management** - Enable/disable, edit content, project-specific overrides\n4. **Learning Tuning** - Confidence decay rates, pattern maturity thresholds\n5. **A/B Testing** - Run prompt variants, measure outcomes\n\nThis transforms hardcoded TypeScript behavior into database-driven, user-tunable configuration.\n\n---\n\n## References\n\n- Durable Streams: https://github.com/durable-streams/durable-streams\n- beads-ui: https://github.com/mantoni/beads-ui\n- beads_viewer: https://github.com/Dicklesworthstone/beads_viewer\n- TanStack Start: https://tanstack.com/start","status":"in_progress","priority":2,"issue_type":"feature","created_at":"2025-12-21T17:14:08.180Z","updated_at":"2025-12-23T17:18:10.968Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j65b49","title":"Create swarm/researcher agent template","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-23T18:32:17.741Z","updated_at":"2025-12-23T18:41:51.421Z","closed_at":"2025-12-23T18:41:51.421Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j6ych1","title":"Add researcher prompt template","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-23T18:32:17.770Z","updated_at":"2025-12-23T18:41:53.462Z","closed_at":"2025-12-23T18:41:53.462Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j730vx","title":"Implement tool discovery for researchers","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T18:32:17.775Z","updated_at":"2025-12-23T18:51:29.505Z","closed_at":"2025-12-23T18:51:29.505Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j77zkw","title":"Implement lockfile parsing for version detection","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T18:32:17.779Z","updated_at":"2025-12-23T19:04:42.600Z","closed_at":"2025-12-23T19:04:42.600Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j7bmjy","title":"Add research phase to swarm orchestration","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-23T18:32:17.783Z","updated_at":"2025-12-23T19:08:57.843Z","closed_at":"2025-12-23T19:08:57.843Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j7enle","title":"Update worker prompt with on-demand research capability","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T18:32:17.786Z","updated_at":"2025-12-23T18:56:42.420Z","closed_at":"2025-12-23T18:56:42.420Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j7ju52","title":"Add --check-upgrades flag support","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T18:32:17.791Z","updated_at":"2025-12-23T19:16:44.200Z","closed_at":"2025-12-23T19:16:44.200Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j7xo6h","title":"Write tests for research phase","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T18:32:17.805Z","updated_at":"2025-12-23T19:16:40.897Z","closed_at":"2025-12-23T19:16:40.897Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjix9j5ssyz","title":"Add Research Phase to /swarm Command","description":"Add a pre-decomposition research phase to the /swarm command that spawns research workers to gather documentation and version information before task decomposition.\n\n## Key Features\n\n1. **swarm/researcher agent type** - New agent that:\n   - Dynamically discovers user's installed tools (MCP servers, skills)\n   - Reads lockfiles to get current package versions\n   - Fetches docs for installed versions using available tools\n   - Stores full findings in semantic-memory\n   - Broadcasts condensed summaries via swarm mail\n   - Uses user's configured worker model (mid-tier)\n\n2. **Pre-decomposition research phase** - Coordinator:\n   - Analyzes task + codebase to identify tech stack\n   - Spawns research workers with explicit tech list\n   - Waits for research to complete\n   - Injects findings into shared_context for all workers\n\n3. **On-demand research in workers** - Update worker prompts to:\n   - Enable spawning research subagents when hitting unknowns\n   - Query semantic memory for existing research first\n\n4. **--check-upgrades flag** - Optional comparison of current vs latest versions\n\n## Architecture\n\n```\nCoordinator receives task\n    ↓\nAnalyze task + codebase → identify tech stack\n    ↓\nSpawn swarm/researcher workers (parallel)\n    ↓\nResearchers: discover tools → read lockfiles → fetch docs → store + broadcast\n    ↓\nCoordinator: collect summaries → inject into shared_context\n    ↓\nNormal decomposition continues with enriched context\n```","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-23T18:32:17.728Z","updated_at":"2025-12-23T19:17:41.150Z","closed_at":"2025-12-23T19:17:41.150Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjfzm2qunwj","title":"Analyze agentic_coding_flywheel_setup for ideas","description":"Analyze https://github.com/Dicklesworthstone/agentic_coding_flywheel_setup for ideas to extract and improve our swarm system.\n\nLook for:\n- Workflow patterns\n- Agent coordination strategies\n- Learning/feedback loops\n- Tool integration patterns\n\n## Research Complete ✓\n\nWorker analyzed the repo and identified 5 key patterns to adopt:\n\n1. **Manifest-Driven Tool Generation** - YAML → TypeScript → tools (4 days)\n2. **Stable Phase IDs** - String IDs instead of array indices for subtasks (1 day)\n3. **Contract Validation** - Pre-flight checks before worker execution (2 days)\n4. **Doctor-Style Health Checks** - 3-tier with caching + timeouts (3 days)\n5. **Checksum-Verified Skills** - Security for external skill downloads (2 days)\n\n**Biggest gap:** No equivalent to `acfs_require_contract()` - workers can call swarm tools without proper initialization.\n\n**ADR:** `.hive/analysis/agentic-flywheel-analysis.md`","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:14:43.686Z","updated_at":"2025-12-24T15:43:42.614Z","closed_at":"2025-12-24T15:43:42.614Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk72toopj7","title":"Add PRAGMA busy_timeout to libSQL connection","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-24T15:54:47.112Z","updated_at":"2025-12-24T15:54:47.112Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk72togddz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk72tor6jx","title":"Create Effect-based withSqliteRetry utility","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-24T15:54:47.115Z","updated_at":"2025-12-24T15:54:47.115Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk72togddz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk6s5lg2g1","title":"Swarm tools need SQLite retry logic for SQLITE_BUSY errors","description":"**Problem:** `hive_sync` and other swarm tools fail with `SQLITE_BUSY: database is locked` instead of retrying.\n\n**Root cause:** Multiple agents/processes accessing the same libSQL database concurrently. SQLite handles this with WAL mode and busy timeouts, but we're not configuring them properly or implementing retry logic.\n\n**Current behavior:**\n```\nhive_sync() → SQLITE_BUSY → immediate failure → user has to retry manually\n```\n\n**Expected behavior:**\n```\nhive_sync() → SQLITE_BUSY → exponential backoff retry (3 attempts) → success or actionable error\n```\n\n**Affected tools:**\n- `hive_sync`, `hive_create`, `hive_update`, `hive_close`\n- `swarmmail_send`, `swarmmail_reserve`, `swarmmail_release`\n- `semantic-memory_store`, `semantic-memory_find`\n- Any tool that writes to libSQL\n\n## Existing Patterns in Codebase\n\nWe already have retry logic in Effect-based code:\n\n1. **`streams/effect/lock.ts`** - Exponential backoff for lock contention:\n```typescript\nconst retrySchedule = Schedule.exponential(baseDelayMs).pipe(\n  Schedule.intersect(Schedule.recurs(maxRetries))\n);\nEffect.retry(retrySchedule)\n```\n\n2. **`memory/ollama.ts`** - Retry for Ollama API calls\n\n3. **`streams/store.ts`** - Idempotency handling for network retries\n\n## Implementation Plan\n\n### Option 1: SQLite busy_timeout (Simplest)\n\nConfigure at connection level - SQLite will automatically retry internally:\n\n```typescript\n// In libsql.ts createLibSQLAdapter()\nconst client = createClient({\n  url: config.url,\n  authToken: config.authToken,\n  // SQLite busy_timeout in milliseconds\n  // Will retry internally for up to 5 seconds\n  intMode: \"number\",\n});\n\n// After connection, set busy_timeout\nawait client.execute(\"PRAGMA busy_timeout = 5000\");\n```\n\n### Option 2: Application-level retry wrapper\n\nWrap all DB operations with retry logic:\n\n```typescript\n// New: src/db/retry.ts\nexport async function withRetry<T>(\n  operation: () => Promise<T>,\n  options: {\n    maxRetries?: number;\n    baseDelayMs?: number;\n    retryableErrors?: string[];\n  } = {}\n): Promise<T> {\n  const { maxRetries = 3, baseDelayMs = 100, retryableErrors = [\"SQLITE_BUSY\"] } = options;\n  \n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      return await operation();\n    } catch (error) {\n      const isRetryable = retryableErrors.some(e => \n        error instanceof Error && error.message.includes(e)\n      );\n      \n      if (!isRetryable || attempt === maxRetries) {\n        throw error;\n      }\n      \n      const delay = baseDelayMs * Math.pow(2, attempt);\n      await new Promise(resolve => setTimeout(resolve, delay));\n    }\n  }\n  throw new Error(\"Unreachable\");\n}\n```\n\n### Option 3: Effect-based retry (Consistent with existing code)\n\nUse Effect's retry combinators like we do in lock.ts:\n\n```typescript\nconst retrySchedule = Schedule.exponential(\"100 millis\").pipe(\n  Schedule.intersect(Schedule.recurs(3)),\n  Schedule.whileInput((error: Error) => \n    error.message.includes(\"SQLITE_BUSY\")\n  )\n);\n\nconst safeQuery = <T>(query: Effect.Effect<T, Error>) =>\n  query.pipe(Effect.retry(retrySchedule));\n```\n\n## Reference: Release It! Patterns\n\nFrom pdf-brain concepts:\n- **Timeout Handling** - Set time limits on operations at integration points\n- **Circuit Breaker Pattern** - Break connections when downstream unavailable\n- **Bulkhead Pattern** - Isolate resources to prevent cascade failures\n\nKey insight: \"Integration points are the number one killer of systems\" - every database call is an integration point that needs protection.\n\n## Recommendation\n\n1. **Immediate fix:** Add `PRAGMA busy_timeout = 5000` to libsql.ts (5 min effort)\n2. **Better fix:** Add `withRetry()` wrapper for all adapter methods (2 hours)\n3. **Best fix:** Migrate to Effect-based retry like lock.ts (1 day)\n\n**Files:**\n- `packages/swarm-mail/src/libsql.ts` - Add busy_timeout PRAGMA\n- `packages/swarm-mail/src/adapter.ts` - Wrap operations with retry\n- Possibly new: `src/db/retry.ts` - Reusable retry utility","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-24T15:46:29.332Z","updated_at":"2025-12-24T16:12:17.663Z","closed_at":"2025-12-24T16:12:17.663Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk72togddz","title":"SQLite Retry Logic - PRAGMA + Effect-based","description":"Implement SQLite retry logic for SQLITE_BUSY errors to handle multi-agent concurrent database access.\n\n## Approach\n1. PRAGMA busy_timeout = 5000 (SQLite-level retry, 5 seconds)\n2. Effect-based retry wrapper using Schedule.exponential() pattern from lock.ts\n3. Apply retry to adapter write operations\n\n## Existing Patterns\n- `streams/effect/lock.ts`: Schedule.exponential(baseDelayMs).pipe(Schedule.compose(Schedule.recurs(maxRetries)))\n- `memory/ollama.ts`: Effect retry for API calls\n\n## Success Criteria\n- PRAGMA busy_timeout set on all new connections\n- Effect-based retry wrapper handles SQLITE_BUSY with exponential backoff\n- Adapter write operations wrapped with retry\n- Tests verify retry behavior\n\n## Related Cell\nCloses: opencode-swarm-monorepo-lf2p4u-mjk6s5lg2g1","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T15:54:47.104Z","updated_at":"2025-12-24T16:12:15.816Z","closed_at":"2025-12-24T16:12:15.816Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk72touuqg","title":"Apply retry wrapper to adapter write operations","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T15:54:47.118Z","updated_at":"2025-12-24T16:12:12.960Z","closed_at":"2025-12-24T16:12:12.960Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk72togddz","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8ee75zkc","title":"Fix Review Loop - Coordinator-Driven Retry","description":"## Problem\n\n`swarm_review_feedback` sends messages to dead workers. Workers are ephemeral Task subagents - they complete and return, then they're gone. The feedback message goes to a mailbox nobody reads.\n\n## Constraint (OpenCode Reality)\n\nWorkers are **fire-and-forget**:\n- Spin up, work, output, die\n- Cannot receive messages after completion\n- Coordinator is locked while worker runs\n- True async/background agents are a future feature (see OpenCode #5887)\n\n## Solution: Coordinator-Driven Retry\n\nWhen `swarm_review_feedback` returns `needs_changes`:\n1. Coordinator reads the issues from response\n2. Coordinator spawns NEW worker with: original task + issues + diff\n3. New worker fixes the issues\n4. Repeat until approved or 3 failures\n\nThe retry loop lives in the **coordinator**, not the worker.\n\n## Key Changes\n\n1. **New tool: `swarm_spawn_retry`** - Generates retry prompt with context\n2. **Update `swarm_review_feedback`** - Return retry context, don't message dead workers\n3. **Update coordinator prompts** - Document retry loop pattern\n4. **ADR** - Document architecture and future considerations\n\n## Success Criteria\n\n- Coordinator can retry failed workers with full context\n- No messages sent to dead workers\n- 3-strike rule actually works\n- Clear documentation of the pattern\n\n## References\n\n- OpenCode Issue #5887: True Async/Background Sub-Agent Delegation (future)\n- Semantic Memory: 2d1dcfa7 (Mandatory Coordinator Review Loop Pattern)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T16:31:46.529Z","updated_at":"2025-12-24T16:33:24.055Z","closed_at":"2025-12-24T16:33:24.055Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8ee7ezb4","title":"Create swarm_spawn_retry tool for retry prompts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:31:46.538Z","updated_at":"2025-12-24T16:33:24.609Z","closed_at":"2025-12-24T16:33:24.609Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8ee75zkc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8ee7g7u0","title":"Update swarm_review_feedback to return retry context","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:31:46.540Z","updated_at":"2025-12-24T16:33:25.499Z","closed_at":"2025-12-24T16:33:25.499Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8ee75zkc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8ee7iqti","title":"Update coordinator post_completion_instructions with retry loop","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:31:46.542Z","updated_at":"2025-12-24T16:33:26.706Z","closed_at":"2025-12-24T16:33:26.706Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8ee75zkc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8ee7kx2b","title":"Update bin/swarm.ts Phase 7 retry documentation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:31:46.544Z","updated_at":"2025-12-24T16:33:27.880Z","closed_at":"2025-12-24T16:33:27.880Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8ee75zkc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8ee7nh7b","title":"Create ADR: Coordinator-Driven Retry Architecture","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T16:31:46.547Z","updated_at":"2025-12-24T16:33:28.808Z","closed_at":"2025-12-24T16:33:28.808Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8ee75zkc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8h0cypll","title":"Add swarm_spawn_retry tool with tests (TDD)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:33:48.562Z","updated_at":"2025-12-24T16:45:13.589Z","closed_at":"2025-12-24T16:45:13.589Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8h0cnppf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8h0d48zt","title":"Update swarm_review_feedback to return retry context","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:33:48.568Z","updated_at":"2025-12-24T16:52:49.819Z","closed_at":"2025-12-24T16:52:49.819Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8h0cnppf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8iblv9ej","title":"Enforce mandatory decomposition validation in coordinator prompt","description":"## Problem\n\nCoordinator skipped `swarm_plan_prompt` → `swarm_validate_decomposition` → `hive_create_epic` flow and went straight to ASCII art planning + direct epic creation. This bypassed:\n- File conflict detection\n- Dependency validation  \n- Strategy selection\n- Structured schema enforcement\n\n## Root Cause\n\nThe coordinator prompt in `bin/swarm.ts` Phase 3 mentions the tools but doesn't make them **mandatory**. Nothing prevents calling `hive_create_epic` directly.\n\n## Fix\n\nUpdate `bin/swarm.ts` Phase 3 (Decompose) to:\n1. Make the flow explicit and non-negotiable\n2. Add \"DO NOT skip these steps\" warning like we have for other phases\n3. Show the exact sequence with tool calls\n4. Explain WHY: validation catches file conflicts before workers get stuck\n\n## Example Update\n\n```markdown\n### Phase 3: Decompose (MANDATORY - DO NOT SKIP)\n\n**⚠️ You MUST use structured decomposition. Do NOT create epics from ASCII art or mental planning.**\n\n1. Generate decomposition prompt:\n   `swarm_plan_prompt(task=\"<task>\", context=\"<knowledge>\", max_subtasks=N)`\n\n2. Respond with CellTree JSON matching the schema\n\n3. Validate before creating cells:\n   `swarm_validate_decomposition(response=\"<your JSON>\")`\n   \n   This catches:\n   - File conflicts (same file in multiple subtasks)\n   - Invalid dependencies\n   - Schema violations\n\n4. ONLY after validation passes:\n   `hive_create_epic(epic_title, subtasks from validated response)`\n\n**Why this matters:** File conflicts cause workers to block on reservations. Validation catches this BEFORE spawning workers.\n```\n\n## Files\n\n- packages/opencode-swarm-plugin/bin/swarm.ts (Phase 3 section)","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-24T16:34:49.795Z","updated_at":"2025-12-24T16:34:49.795Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8h0cnppf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8h0cnppf","title":"Fix Review Loop - Coordinator-Driven Retry","description":"Workers are fire-and-forget. swarm_review_feedback sends messages to dead workers. Fix by making coordinator drive the retry loop - spawn new workers with feedback context instead of messaging dead ones.\n\n## Constraint (OpenCode Reality)\n\nWorkers are ephemeral Task subagents:\n- Spin up, work, output, die\n- Cannot receive messages after completion\n- Coordinator is locked while worker runs\n- True async/background agents are future (OpenCode #5887)\n\n## Solution\n\nWhen `swarm_review_feedback` returns `needs_changes`:\n1. Coordinator calls `swarm_spawn_retry()` to generate retry prompt\n2. Coordinator spawns NEW worker with Task() using that prompt\n3. New worker has: original task + issues + diff context\n4. Repeat until approved or 3 failures\n\n## Validated Decomposition\n\nStrategy: risk-based (TDD first, isolate changes)\nTotal complexity: 11 | Files: 6 | Subtasks: 4","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T16:33:48.551Z","updated_at":"2025-12-24T17:01:14.770Z","closed_at":"2025-12-24T17:01:14.770Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8h0d8rzl","title":"Update bin/swarm.ts Phase 7 retry documentation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:33:48.572Z","updated_at":"2025-12-24T17:01:05.308Z","closed_at":"2025-12-24T17:01:05.308Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8h0cnppf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8h0da1zn","title":"Create ADR: Coordinator-Driven Retry Architecture","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T16:33:48.574Z","updated_at":"2025-12-24T17:01:07.047Z","closed_at":"2025-12-24T17:01:07.047Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk8h0cnppf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk941x3gqy","title":"BUG: Coordinator does work directly after compaction instead of spawning workers","description":"## Critical Bug\n\n**Symptom:** After context compaction, the coordinator starts doing work directly instead of spawning worker agents. This breaks the entire swarm coordination model.\n\n**Root Cause Hypothesis:** \nWhen compaction happens mid-swarm, the coordinator loses:\n1. Its identity as \"coordinator\" (role context)\n2. The instruction to spawn workers via Task()\n3. The swarm state (which workers are active, what's been done)\n\nThe compacted context doesn't preserve the critical \"I am a COORDINATOR, I spawn workers, I don't do work\" instruction.\n\n**Evidence:**\n- Worker was spawned for subtask 2 (swarm_review_feedback retry context)\n- Worker completed successfully\n- But coordinator never reviewed the work or spawned next worker\n- Instead, coordinator context was lost and it started acting like a general agent\n\n**Current compaction hook gap:**\nThe hook injects generic \"you are a coordinator\" context but doesn't include:\n1. The SPECIFIC epic ID being coordinated\n2. Which subtasks are done/pending/in_progress  \n3. The original task description\n4. Which workers were spawned\n\nThe agent wakes up knowing it's a coordinator but not WHAT it's coordinating.\n\n**Next steps:**\n1. Set up o11y for compaction - capture what context exists before/after\n2. Create eval for coordinator resumption after compaction\n3. Analyze compaction logs to understand failure mode\n4. Fix compaction hook to include actual swarm state\n\n**Files:**\n- packages/opencode-swarm-plugin/src/compaction-hook.ts\n- packages/opencode-swarm-plugin/src/compaction-hook.test.ts\n\n**Failing tests added** (TDD red phase) - 3 tests expecting specific epic ID, subtask status, and project path in compaction context.","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-24T16:51:43.671Z","updated_at":"2025-12-24T17:17:36.321Z","closed_at":"2025-12-24T17:17:36.321Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk9k00s34c","title":"Fix Compaction Bug: Coordinator Identity Loss","description":"Investigate and fix P0 bug where coordinator loses identity after context compaction. Three parallel tracks: O11y to capture failure mode, Eval to define success criteria, Fix to implement the solution. Risk-based approach: tests/evals first, then fix.\n\nRoot cause: The compaction hook injects generic \"you are a coordinator\" context but doesn't include the SPECIFIC epic ID, subtask status, or project path. Agent wakes up knowing it's a coordinator but not WHAT it's coordinating.\n\nRelated bug cell: mjk941x3gqy","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T17:04:07.708Z","updated_at":"2025-12-24T17:17:34.978Z","closed_at":"2025-12-24T17:17:34.978Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk9k00y1mx","title":"Eval: Create coordinator resumption eval","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T17:04:07.714Z","updated_at":"2025-12-24T17:17:27.666Z","closed_at":"2025-12-24T17:17:27.666Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk9k00s34c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk9k010mvr","title":"TDD: Add failing tests for specific swarm state in compaction context","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T17:04:07.716Z","updated_at":"2025-12-24T17:17:29.087Z","closed_at":"2025-12-24T17:17:29.087Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk9k00s34c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk9k013xq3","title":"Fix: Inject specific swarm state into compaction context with enhanced o11y","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T17:04:07.719Z","updated_at":"2025-12-24T17:17:11.098Z","closed_at":"2025-12-24T17:17:11.098Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjk9k00s34c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkb2jksxw8","title":"Add scanSessionMessages function with SDK client types","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T17:46:32.476Z","updated_at":"2025-12-24T17:52:18.585Z","closed_at":"2025-12-24T17:52:18.585Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkb2jklhzc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkhw448ge8","title":"Coordinator Observability: Eval-Compatible Logging","description":"Add comprehensive coordinator logging that captures decision points, detects violations, and outputs evalite-compatible session data for scoring and prompt improvement. Stores in ~/.config/swarm-tools/sessions/","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T20:57:29.816Z","updated_at":"2025-12-24T21:22:37.984Z","closed_at":"2025-12-24T21:22:37.984Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkhw44l8f8","title":"Define CoordinatorEvent schemas and session capture infrastructure","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T20:57:29.829Z","updated_at":"2025-12-24T21:07:02.379Z","closed_at":"2025-12-24T21:07:02.379Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkhw448ge8","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkhw44skfy","title":"Add violation detection to planning guardrails","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T20:57:29.836Z","updated_at":"2025-12-24T21:16:17.072Z","closed_at":"2025-12-24T21:16:17.072Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkhw448ge8","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkhw44u7ew","title":"Instrument swarm coordination tools with decision tracing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T20:57:29.838Z","updated_at":"2025-12-24T21:16:18.280Z","closed_at":"2025-12-24T21:16:18.280Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkhw448ge8","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkhw44xyf5","title":"Create coordinator-discipline scorer for evalite","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T20:57:29.841Z","updated_at":"2025-12-24T21:16:19.391Z","closed_at":"2025-12-24T21:16:19.391Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkhw448ge8","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkhw44zecc","title":"Add coordinator-session eval that scores real captured sessions","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T20:57:29.843Z","updated_at":"2025-12-24T21:22:31.257Z","closed_at":"2025-12-24T21:22:31.257Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkhw448ge8","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkj8utjcmi","title":"Add PGlite deprecation warnings - Phase 1","description":"Add console.warn deprecation notices to all PGlite-related functions. Uses shared flag to log once per session. Prepares users for PGlite removal in next major version.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T21:35:23.911Z","updated_at":"2025-12-24T21:43:55.828Z","closed_at":"2025-12-24T21:43:55.828Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkj8uts811","title":"Add deprecation warning helper and flag","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T21:35:23.920Z","updated_at":"2025-12-24T21:41:49.208Z","closed_at":"2025-12-24T21:41:49.208Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkj8utjcmi","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkj8utwoep","title":"Add deprecation warning to toDrizzleDb PGlite branch","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T21:35:23.924Z","updated_at":"2025-12-24T21:42:58.942Z","closed_at":"2025-12-24T21:42:58.942Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkj8utjcmi","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkj8uu097q","title":"Add deprecation warning to migratePGliteToLibSQL","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T21:35:23.928Z","updated_at":"2025-12-24T21:43:50.325Z","closed_at":"2025-12-24T21:43:50.325Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkj8utjcmi","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkj8uu5awv","title":"Add changeset for deprecation notice","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T21:35:23.933Z","updated_at":"2025-12-24T21:41:50.832Z","closed_at":"2025-12-24T21:41:50.832Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkj8utjcmi","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkjqozr6ak","title":"Fix compaction context to enforce coordinator discipline","description":"## Problem\n\nAfter compaction, coordinator ignores explicit \"spawn workers\" instructions in session summary and does work directly.\n\n**Evidence:** Session where I wrote my own summary saying \"spawn workers for remaining subtasks\" and then immediately edited files myself.\n\n## Root Cause Analysis\n\n### 1. Compaction Context is Narrative, Not Prescriptive\n\nThe `SWARM_COMPACTION_CONTEXT` in `compaction-hook.ts`:\n- Says what TO do, not what NOT to do\n- \"Spawn ready subtasks\" is step 4, easily skipped\n- Missing explicit prohibitions\n\n### 2. Violation Detection Exists But Isn't Wired Up\n\nWe have `detectCoordinatorViolation()` in `planning-guardrails.ts` that can detect:\n- `coordinator_edited_file` - when Edit/Write tools are used\n- `coordinator_ran_tests` - when bash runs test commands\n- `coordinator_reserved_files` - when swarmmail_reserve is called\n- `no_worker_spawned` - when epic created without spawning\n\n**BUT** it's never called from the plugin! It's only used in tests.\n\nSession capture at `~/.config/swarm-tools/sessions/` shows only DECISION events, no VIOLATION events - because we never call the detection.\n\n### 3. No Context for \"Am I Coordinator?\"\n\nEven if we wire up detection, we need to know if we're in coordinator context. Options:\n- Check for active epic in hive\n- Check if swarmmail was initialized  \n- Track state in plugin\n\n## Proposed Fix\n\n### Phase 1: Wire Up Violation Detection\n\nIn `src/index.ts`, update `tool.execute.before`:\n\n```typescript\nimport { detectCoordinatorViolation } from \"./planning-guardrails\";\n\n\"tool.execute.before\": async (input, output) => {\n  // ... existing todowrite check ...\n  \n  // Check for coordinator violations\n  const epicId = await getActiveEpicId(); // Need to implement\n  if (epicId) {\n    const violation = detectCoordinatorViolation({\n      sessionId: input.sessionID,\n      epicId,\n      toolName: input.tool,\n      toolArgs: output.args,\n      agentContext: \"coordinator\",\n    });\n    \n    if (violation.isViolation) {\n      console.warn(`[swarm-plugin] ${violation.message}`);\n      // Event already captured by detectCoordinatorViolation\n    }\n  }\n};\n```\n\n### Phase 2: Restructure Compaction Context\n\nChange from narrative to checklist with prohibitions first:\n\n```markdown\n## 🐝 SWARM ACTIVE - You Are The COORDINATOR\n\n### ⛔ NEVER (Coordinator Anti-Patterns)\n- [ ] Edit files directly - SPAWN A WORKER\n- [ ] Run tests yourself - SPAWN A WORKER  \n- [ ] Implement features - SPAWN A WORKER\n- [ ] \"Just do it myself\" - NO. SPAWN A WORKER.\n\n### ✅ ALWAYS (Coordinator Checklist)\n1. [ ] `swarm_status(epic_id=\"...\", project_key=\"...\")`\n2. [ ] `swarmmail_inbox(limit=5)`\n3. [ ] For completed work: `swarm_review` → `swarm_review_feedback`\n4. [ ] For open subtasks: `swarm_spawn_subtask` (NOT \"do it yourself\")\n5. [ ] For blocked work: Investigate, unblock, reassign\n```\n\n## Files\n- `packages/opencode-swarm-plugin/src/index.ts` - Wire up violation detection\n- `packages/opencode-swarm-plugin/src/compaction-hook.ts` - Restructure context\n- `packages/opencode-swarm-plugin/src/planning-guardrails.ts` - Add getActiveEpicId()\n\n## Validation\n- Check `~/.config/swarm-tools/sessions/` for VIOLATION events after fix\n- Run `coordinator-behavior.eval.ts` before/after\n- Run `compaction-resumption.eval.ts` to ensure detection still works","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-24T21:49:16.167Z","updated_at":"2025-12-24T22:05:11.381Z","closed_at":"2025-12-24T22:05:11.381Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkk22h6ogw","title":"Cell IDs not stable during sync - breaks references","description":"**Problem:** Cell IDs are changing during hive_sync, breaking all references.\n\n**Evidence:** Cell `mjkjqozr6ak` (coordinator discipline fix) no longer exists after sync. Query returns different IDs for same cells.\n\n**Impact:**\n- Cross-references in descriptions break\n- ADRs referencing cell IDs become stale\n- Swarm coordination breaks (epic_id, parent_id references)\n- Thread IDs in swarm mail become orphaned\n\n**Expected:** Cell IDs are immutable once created. Sync should merge/update, not regenerate IDs.\n\n**Investigation needed:**\n1. How are IDs generated? (check `generateBeadId` or equivalent)\n2. What happens during sync merge?\n3. Is this a git merge conflict resolution issue?\n4. Are IDs being regenerated on parse?\n\n**Files to check:**\n- `packages/swarm-mail/src/hive/` - cell storage and sync logic\n- ID generation functions","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-24T21:58:06.858Z","updated_at":"2025-12-24T21:59:53.028Z","closed_at":"2025-12-24T21:59:53.028Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhgw0ge029","title":"Update root README - human-forward quickstart","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-22T18:06:06.926Z","updated_at":"2025-12-24T22:57:43.361Z","closed_at":"2025-12-24T22:57:43.361Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhgw0g8l1v","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhgw0ggt00","title":"Update opencode-swarm-plugin README - plugin quickstart","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-22T18:06:06.928Z","updated_at":"2025-12-24T22:57:44.554Z","closed_at":"2025-12-24T22:57:44.554Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhgw0g8l1v","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhgw0gie5o","title":"Update swarm-mail README - library usage","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T18:06:06.930Z","updated_at":"2025-12-24T22:57:51.469Z","closed_at":"2025-12-24T22:57:51.469Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhgw0g8l1v","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhgw0gk8wh","title":"Update docs site quickstart.mdx - THE main doc","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-22T18:06:06.932Z","updated_at":"2025-12-24T22:57:45.637Z","closed_at":"2025-12-24T22:57:45.637Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhgw0g8l1v","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhgw0godfy","title":"Update docs site index.mdx - landing page","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T18:06:06.936Z","updated_at":"2025-12-24T22:57:52.617Z","closed_at":"2025-12-24T22:57:52.617Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhgw0g8l1v","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkmdasw7rf","title":"Fix cell detection and short ID resolution bugs","description":"Two related bugs preventing proper hive operation:\n\n1. **Compaction hook cell detection** - CLI returns `{success, data}` wrapper but detectSwarm() expects raw array. 13KB of valid cell data is being discarded.\n\n2. **Short ID resolution** - `resolvePartialId` fails to match partial IDs like `mjhgw0ge029`, requiring full IDs.\n\nRoot cause hypothesis: Response format handling issues in both cases.\n\n**Priority:** Cell detection first (blocks coordinator discipline enforcement), then short ID.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T23:02:50.096Z","updated_at":"2025-12-24T23:09:57.522Z","closed_at":"2025-12-24T23:09:57.522Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkmdat26vq","title":"Fix detectSwarm to unwrap {success, data} response","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T23:02:50.102Z","updated_at":"2025-12-24T23:09:05.112Z","closed_at":"2025-12-24T23:09:05.112Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkmdasw7rf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkmdat5rip","title":"Fix resolvePartialId to handle partial hashes correctly","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T23:02:50.105Z","updated_at":"2025-12-24T23:09:06.780Z","closed_at":"2025-12-24T23:09:06.780Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkmdasw7rf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkmdat77ff","title":"Rename plugin-wrapper-template.ts to clarify it IS the plugin","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T23:02:50.107Z","updated_at":"2025-12-24T23:09:51.805Z","closed_at":"2025-12-24T23:09:51.805Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkmdasw7rf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjggwznl7gx","title":"Add PGlite deprecation warnings, remove in next major","description":"## Context\nPGlite migration code remains for backward compatibility but libSQL is now the only supported database.\n\n## What to do\n\n### Phase 1: Add Deprecation Warnings (this release)\n- `wrapPGlite()` in pglite.ts - add console.warn on first call\n- `toDrizzleDb()` PGlite branch - add console.warn when PGlite detected\n- `migratePGliteToLibSQL()` - add warning that this is last supported version\n- Update CHANGELOG with deprecation notice\n\n### Phase 2: Remove in Next Major (v1.0 or v0.32)\n- Remove pglite.ts entirely\n- Remove PGlite branch from toDrizzleDb()\n- Remove migrate-pglite-to-libsql.ts\n- Remove memory/migrate-legacy.ts PGlite code\n- Remove @electric-sql/pglite from dependencies\n- Remove drizzle-orm/pglite imports\n\n## Files affected\n- packages/swarm-mail/src/pglite.ts\n- packages/swarm-mail/src/libsql.convenience.ts\n- packages/swarm-mail/src/migrate-pglite-to-libsql.ts\n- packages/swarm-mail/src/memory/migrate-legacy.ts\n- packages/swarm-mail/package.json (remove dep)","status":"closed","priority":2,"issue_type":"chore","created_at":"2025-12-22T01:19:06.369Z","updated_at":"2025-12-24T23:24:32.718Z","closed_at":"2025-12-24T23:24:32.718Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjgpm1e5zjq","title":"sendSwarmMessage needs dbAdapter parameter for test isolation","description":"**Problem:** `sendSwarmMessage` in swarm-review.ts creates its own LibSQLAdapter internally, which fails with \"URL_INVALID\" for paths like `~/.config/swarm-tools/swarm.db`.\n\n**Impact:** \n- swarm_review_feedback integration tests can't use in-memory databases\n- Tests are skipped in swarm-review.integration.test.ts\n\n**Root cause:**\nsendSwarmMessage calls getSwarmMailLibSQL() without accepting a dbAdapter parameter, so it can't use the test's in-memory adapter.\n\n**Fix options:**\n1. Add dbAdapter parameter to swarm_review_feedback (breaking change)\n2. Make sendSwarmMessage use adapter cache (global state)\n3. Use file:// URLs for all database paths\n\n**Files:**\n- packages/opencode-swarm-plugin/src/swarm-review.ts\n- packages/opencode-swarm-plugin/src/swarm-review.integration.test.ts","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-22T05:22:31.949Z","updated_at":"2025-12-24T23:24:36.354Z","closed_at":"2025-12-24T23:24:36.354Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkmdyoqhn4","title":"Add `swarm cells` CLI command with intuitive filtering","description":"Replace awkward `swarm tool hive_query` with intuitive `swarm cells` command that reads DIRECTLY from libSQL database.\n\n## Current Problems\n1. `swarm tool hive_query` - wtf is this syntax\n2. Reads from JSONL files instead of database\n3. Returns wrapped `{success, data}` format that breaks parsing\n\n## Proposed\n```bash\nswarm cells                      # list all cells (from DB)\nswarm cells --status open        # filter by status\nswarm cells --status in_progress # WIP\nswarm cells --type epic          # filter by type\nswarm cells mjhgw0g              # get by partial ID\nswarm cells --ready              # next unblocked cell\nswarm cells --json               # raw JSON array output\n```\n\n## Implementation\n- Add `cells` subcommand to bin/swarm.ts\n- Use `getSwarmMailLibSQL()` → `adapter.queryCells()` directly\n- NO JSONL parsing - database is source of truth\n- Support partial ID resolution via `resolvePartialId()`\n- Default to human-readable table output\n- `--json` outputs raw array (no wrapper)\n\n## Why Database > JSONL\n- JSONL is for git sync/export, not querying\n- Database has indexes, fast queries\n- Database is always up-to-date\n- JSONL can be stale until sync\n\n## Fixes\nThis also fixes detectSwarm - call `adapter.queryCells()` directly instead of spawning CLI subprocess.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-24T23:03:21.050Z","updated_at":"2025-12-24T23:24:34.415Z","closed_at":"2025-12-24T23:24:34.415Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkmdasw7rf","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhji3rrl06","title":"Observability Research Spike - ADRs","description":"Research spike to explore observability options for swarm tools. Output: 3 focused ADRs covering runtime visibility, post-hoc analysis, and developer debugging. No production code - design docs and feasibility assessment only.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-22T19:19:16.887Z","updated_at":"2025-12-24T23:33:56.253Z","closed_at":"2025-12-24T23:33:56.253Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhji3rzhum","title":"ADR: Post-Hoc Analysis - Traces, Analytics & Pattern Extraction","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:19:16.895Z","updated_at":"2025-12-24T23:33:53.944Z","closed_at":"2025-12-24T23:33:53.944Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhji3rrl06","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjhji3s2pb3","title":"ADR: Developer Debugging - Verbose Modes, Replay & State Dumps","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-22T19:19:16.898Z","updated_at":"2025-12-24T23:33:54.905Z","closed_at":"2025-12-24T23:33:54.905Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjhji3rrl06","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk7ztm5jmu","title":"Research: Worker retry loop for review feedback","description":"## Problem\n\n`swarm_review_feedback` sends messages to dead workers. Workers are ephemeral Task subagents - they complete and return, then they're gone. The feedback message goes to a mailbox nobody reads.\n\n## Current Flow (Broken)\n\n```\nCoordinator spawns Worker → Worker works → Worker returns result\n                                                    ↓\n                                          Coordinator reviews diff\n                                                    ↓\n                                          swarm_review_feedback(needs_changes)\n                                                    ↓\n                                          Message sent to dead worker 💀\n```\n\n## What Should Happen\n\nWhen `swarm_review_feedback` returns `needs_changes`, the **coordinator** should:\n1. Read the issues from the response\n2. Spawn a NEW worker with: original task + issues + \"fix these\"\n3. Repeat until approved or 3 failures\n\nThe retry loop belongs in the **coordinator**, not the worker.\n\n## Research Questions\n\n1. **Session continuity**: Can we use `session_id` in Task tool to continue a worker session?\n2. **Prompt injection**: Should feedback be injected into a new worker prompt?\n3. **State preservation**: How do we pass \"attempt 2 of 3\" context to new worker?\n4. **Diff context**: Should new worker see the diff of what previous worker did?\n\n## Options to Explore\n\n### Option A: Coordinator-driven retry loop\n```\nwhile (attempts < 3) {\n  result = await Task(worker_prompt + previous_feedback)\n  review = swarm_review(...)\n  if (approved) break\n  feedback = swarm_review_feedback(needs_changes, issues)\n  attempts++\n}\n```\n\n### Option B: Session continuation\n```\nresult = await Task(..., session_id=\"worker-123\")\n// If needs_changes:\nresult = await Task(\"Fix these issues: ...\", session_id=\"worker-123\")  // Same session\n```\n\n### Option C: Persistent worker agents\n- Workers don't return until approved\n- Workers check inbox periodically\n- Coordinator sends feedback, worker reads and retries\n- More complex, requires worker polling loop\n\n## Files to Investigate\n\n- `src/swarm-review.ts` - current feedback mechanism\n- `src/swarm-prompts.ts` - worker prompt generation\n- `src/swarm-orchestrate.ts` - coordinator workflow\n- OpenCode Task tool docs - session_id behavior\n\n## Success Criteria\n\n- Design doc with recommended approach\n- Clear implementation plan\n- Addresses: state preservation, attempt tracking, feedback injection\n- Considers: context efficiency, error recovery, learning signals","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:20:26.669Z","updated_at":"2025-12-24T23:33:57.537Z","closed_at":"2025-12-24T23:33:57.537Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkb2jklhzc","title":"Enhanced compaction hook with SDK client message scanning","description":"Scan actual session messages for swarm tool calls to build PRECISE context for resumed coordinators. Current approach only uses hive cells and swarm-mail health. Enhanced approach scans swarm_spawn_subtask, swarm_status, hive_create_epic, swarmmail_init calls to extract exact epic IDs, subtask definitions, agent names, and worker assignments.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T17:46:32.469Z","updated_at":"2025-12-24T23:34:25.244Z","closed_at":"2025-12-24T23:34:25.244Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkb2jku9gi","title":"Update createCompactionHook to accept SDK client and merge scanned state","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T17:46:32.478Z","updated_at":"2025-12-24T23:34:13.878Z","closed_at":"2025-12-24T23:34:13.878Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkb2jklhzc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkb2jkwe76","title":"Register compaction hook in SwarmPlugin with client","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T17:46:32.480Z","updated_at":"2025-12-24T23:34:14.773Z","closed_at":"2025-12-24T23:34:14.773Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkb2jklhzc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkb2jky5lw","title":"Add tests for message scanning and state merging","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-24T17:46:32.482Z","updated_at":"2025-12-24T23:34:23.941Z","closed_at":"2025-12-24T23:34:23.941Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkb2jklhzc","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkma1cu7lh","title":"Short ID resolution regression - hive_close fails with partial IDs","description":"## Problem\n\nShort cell IDs (e.g., `mjhgw0ge029`) fail to resolve in `hive_close` and other hive tools, requiring full IDs like `opencode-swarm-monorepo-lf2p4u-mjhgw0ge029`.\n\nThis is a regression - short ID resolution was working before.\n\n## Evidence\n\n```\nhive_close(id=\"mjhgw0ge029\", reason=\"...\")\n→ Error: No cell found matching ID 'mjhgw0ge029'\n\nhive_close(id=\"opencode-swarm-monorepo-lf2p4u-mjhgw0ge029\", reason=\"...\")\n→ Success\n```\n\n## Investigation\n\n1. `resolvePartialId` exists in `swarm-mail/src/hive/queries.ts` and is exported\n2. `hive.ts` imports and calls it at line 979: `const cellId = await resolvePartialId(adapter, projectKey, validated.id) || validated.id;`\n3. Tests exist in `queries.test.ts` and pass\n\n## Hypothesis\n\nEither:\n1. `resolvePartialId` is returning null when it shouldn't\n2. The adapter being passed doesn't have the right database connection\n3. The projectKey doesn't match what's in the database\n\n## Files\n\n- packages/opencode-swarm-plugin/src/hive.ts (line 979)\n- packages/swarm-mail/src/hive/queries.ts\n- packages/swarm-mail/src/hive/queries-drizzle.ts","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-24T23:00:17.886Z","updated_at":"2025-12-24T23:33:21.703Z","closed_at":"2025-12-24T23:33:21.703Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkma8zt7dg","title":"Compaction hook cell detection fails despite cells existing","description":"## Problem\n\nThe compaction hook's swarm detection reports \"no cells found\" even when cells clearly exist. This prevents the prohibition-first coordinator context from being injected.\n\n## Evidence from logs\n\n```json\n{\"msg\":\"detect_swarm_cli_complete\",\"exit_code\":0,\"stdout_length\":13185}\n{\"msg\":\"detect_swarm_no_cells\",\"is_array\":false,\"length\":0}\n{\"msg\":\"swarm_detection_complete\",\"detected\":false,\"confidence\":\"none\",\"reasons\":[\"no cells found\"]}\n```\n\nThe CLI returns 13KB of data (exit_code=0, stdout_length=13185) but the parsing step produces `is_array=false, length=0`.\n\n## Impact\n\n1. Session scanning finds 60+ high-confidence swarm tools (`hive_create_epic`, `swarm_spawn_subtask`, etc.)\n2. But hive detection fails → `effectiveConfidence` stays \"none\"\n3. The `SWARM_COMPACTION_CONTEXT` with prohibition-first rules is NOT injected\n4. Coordinator resumes without the \"NEVER edit files directly\" guardrails\n\n## Root Cause Hypothesis\n\nThe detection code is calling some CLI or adapter method that returns data, but:\n1. The response format changed and parsing is broken\n2. The adapter is querying the wrong database/project\n3. The response is not being parsed as JSON correctly\n\n## Files\n\n- packages/opencode-swarm-plugin/src/compaction-hook.ts (detectSwarm function around line 578)\n- Look for log statements: `detect_swarm_cli_complete`, `detect_swarm_no_cells`\n\n## Related\n\nThis may be related to the short ID resolution regression - both suggest the adapter/database connection is misconfigured.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-24T23:00:27.785Z","updated_at":"2025-12-24T23:34:36.965Z","closed_at":"2025-12-24T23:34:36.965Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk6htl0c24","title":"Swarm coordinator should spawn research workers, not fetch files directly","description":"**Problem:** Coordinator is doing research grunt work (repo-crawl_file, fetching docs) instead of spawning a research worker to do it.\n\n**Root cause:** The swarm prompts don't enforce the coordinator/worker separation strongly enough. Coordinator prompt should:\n1. NEVER call repo-crawl_*, webfetch, or other research tools directly\n2. ALWAYS spawn a `swarm-researcher` or `explore` agent for information gathering\n3. Only synthesize results returned by workers\n\n**Current behavior:**\n- Coordinator calls `repo-crawl_file` 10+ times directly\n- Burns coordinator context on raw file contents\n- Doesn't leverage parallel research workers\n\n**Expected behavior:**\n```\nCoordinator:\n  1. swarm_spawn_subtask(type=\"research\", task=\"Analyze ACFS workflow patterns\")\n  2. swarm_spawn_subtask(type=\"research\", task=\"Analyze ACFS agent coordination\")\n  3. Wait for workers to return summaries\n  4. Synthesize into ADR\n```\n\n**Fix locations:**\n- `src/swarm-prompts.ts` - coordinator prompt needs explicit \"NEVER fetch directly\" rule\n- `src/swarm-orchestrate.ts` - research phase should spawn workers, not run inline\n- Consider adding a `swarm_spawn_researcher` convenience tool\n\n**Related:** Research phase was just added but it runs inline in coordinator context instead of spawning workers.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-24T15:38:27.204Z","updated_at":"2025-12-24T23:41:58.914Z","closed_at":"2025-12-24T23:41:58.914Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjknrislmka","title":"Coordinator spawns research workers instead of fetching directly","description":"Fix coordinator to delegate research to workers. Extract COORDINATOR_PROMPT to swarm-prompts.ts with explicit 'NEVER fetch directly' rules. Update compaction hook with same rules + REAL FUCKIN SURE coordinator identity. Wire runResearchPhase to spawn researcher workers.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T23:41:53.253Z","updated_at":"2025-12-24T23:55:10.648Z","closed_at":"2025-12-24T23:55:10.648Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjknrisvqty","title":"Extract COORDINATOR_PROMPT with 'NEVER fetch directly' rules","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T23:41:53.264Z","updated_at":"2025-12-24T23:50:02.038Z","closed_at":"2025-12-24T23:50:02.038Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjknrislmka","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjknriszys7","title":"Add 'NEVER fetch directly' rules to compaction hook + STRONG coordinator identity","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T23:41:53.267Z","updated_at":"2025-12-24T23:50:03.316Z","closed_at":"2025-12-24T23:50:03.316Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjknrislmka","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjknrit3lk2","title":"Wire runResearchPhase to spawn researcher workers","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T23:41:53.271Z","updated_at":"2025-12-24T23:50:04.083Z","closed_at":"2025-12-24T23:50:04.083Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjknrislmka","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjknrit5e93","title":"Update bin/swarm.ts to use extracted COORDINATOR_PROMPT","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T23:41:53.273Z","updated_at":"2025-12-24T23:55:02.535Z","closed_at":"2025-12-24T23:55:02.535Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjknrislmka","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkvvke300y","title":"Post-compaction must inject full COORDINATOR_PROMPT, not just identity","description":"**Problem:** Post-compaction context only has identity reminders and anti-patterns, but NOT the full coordinator workflow (phases, tools, review loop).\n\n**Current state:**\n- SWARM_COMPACTION_CONTEXT has ASCII header + anti-patterns + checklist\n- But missing: Phase 1.5 Research, forbidden tools details, full workflow\n\n**Required:** Post-compaction agent should get the FULL COORDINATOR_PROMPT injected, not a summary. We are ALWAYS swarming after compaction.\n\n**Fix:**\n1. compaction-hook.ts: Inject COORDINATOR_PROMPT directly\n2. plugin-wrapper-template.ts: LLM continuation prompt should include full ruleset\n3. The agent post-compaction IS the coordinator - give it the full playbook","status":"closed","priority":0,"issue_type":"bug","created_at":"2025-12-25T03:28:58.875Z","updated_at":"2025-12-25T03:31:50.497Z","closed_at":"2025-12-25T03:31:50.497Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw2iwso0s","title":"Capture compaction prompts and results for eval system","description":"**Problem:** Compaction hook generates LLM prompts and injects context, but we have no visibility into:\n1. What prompts are being generated\n2. What context is being injected\n3. Whether the coordinator actually resumes correctly\n\n**Solution:** Hook compaction logs into eval-capture system.\n\n---\n\n## Current State Analysis\n\n### What We Have (from logs)\n\nThe compaction system is **working** - here's proof from `~/.config/swarm-tools/logs/compaction.log`:\n\n```\nSession: ses_4b86f0867ffeXKv95ktf31igfD\n\n1. SESSION SCAN (1.6s)\n   - 1565 messages scanned\n   - 261 tool calls found\n   - 73 HIGH-CONFIDENCE swarm tools detected\n     └─ swarmmail_init, hive_create_epic, swarm_spawn_subtask\n   - swarm_detected: TRUE\n\n2. HIVE DETECTION (0.6s)\n   - 20 cells found\n   - confidence: \"none\" (no in_progress epics)\n\n3. CONFIDENCE BOOST\n   - Original: \"none\" (from hive)\n   - Boosted to: \"high\" (from session scan)\n   - Reason: 73 high-confidence swarm tools\n\n4. LLM GENERATION (17.6s)\n   - Model: anthropic/claude-haiku-4-5\n   - Generated 4533 char prompt\n   - Includes: 🐝 YOU ARE THE COORDINATOR 🐝\n\n5. CONTEXT INJECTED ✓\n   - 4672 chars via output.prompt\n\nTOTAL: 20.8s\n```\n\n### Log Structure (existing)\n\n```typescript\n// From plugin-wrapper-template.ts\nlogCompaction(\"info\", \"session_scan_complete\", {\n  session_id, duration_ms, message_count, tool_call_count,\n  swarm_tool_count, high_confidence_count, swarm_detected, reasons, unique_tools\n});\n\nlogCompaction(\"info\", \"swarm_detection_complete\", {\n  session_id, duration_ms, detected, confidence, reasons, reason_count\n});\n\nlogCompaction(\"info\", \"confidence_boost_from_session_scan\", {\n  session_id, original_confidence, boosted_to, session_reasons\n});\n\nlogCompaction(\"info\", \"llm_generation_complete\", {\n  session_id, duration_ms, success, prompt_length, prompt_preview\n});\n\nlogCompaction(\"info\", \"context_injected_via_prompt_api\", {\n  session_id, content_length, method\n});\n\nlogCompaction(\"info\", \"compaction_complete_llm_success\", {\n  session_id, total_duration_ms, detection_duration_ms, query_duration_ms,\n  llm_duration_ms, confidence, context_type, content_length\n});\n```\n\n### What's Missing\n\n1. **Full prompt content** - Only `prompt_preview` (first 500 chars) is logged\n2. **Structured capture** - Logs are text, not queryable for evals\n3. **Eval integration** - No scorers for prompt quality\n4. **Outcome tracking** - Did coordinator actually resume correctly?\n\n---\n\n## Existing Eval Infrastructure\n\n### eval-capture.ts Schemas\n\n```typescript\n// CoordinatorEvent - discriminated union\nexport const CoordinatorEventSchema = z.discriminatedUnion(\"event_type\", [\n  // DECISION events\n  z.object({\n    session_id: z.string(),\n    epic_id: z.string(),\n    timestamp: z.string(),\n    event_type: z.literal(\"DECISION\"),\n    decision_type: z.enum([\n      \"strategy_selected\", \"worker_spawned\", \n      \"review_completed\", \"decomposition_complete\"\n    ]),\n    payload: z.any(),\n  }),\n  // VIOLATION events\n  z.object({\n    session_id: z.string(),\n    epic_id: z.string(),\n    timestamp: z.string(),\n    event_type: z.literal(\"VIOLATION\"),\n    violation_type: z.enum([\n      \"coordinator_edited_file\", \"coordinator_ran_tests\",\n      \"coordinator_reserved_files\", \"no_worker_spawned\"\n    ]),\n    payload: z.any(),\n  }),\n  // OUTCOME events\n  z.object({\n    session_id: z.string(),\n    epic_id: z.string(),\n    timestamp: z.string(),\n    event_type: z.literal(\"OUTCOME\"),\n    outcome_type: z.enum([\n      \"subtask_success\", \"subtask_retry\",\n      \"subtask_failed\", \"epic_complete\"\n    ]),\n    payload: z.any(),\n  }),\n]);\n```\n\n### Existing Scorers (evals/scorers/)\n\n- `subtaskIndependence` - No file overlap between subtasks\n- `complexityBalance` - CV of estimated_complexity < 0.3\n- `coverageCompleteness` - Required files covered\n- `instructionClarity` - Description length, files specified\n- `confidenceAccuracy` - Detection confidence matches expected\n- `contextInjectionCorrectness` - Right context type injected\n- `requiredPatternsPresent` - Must contain certain strings\n- `forbiddenPatternsAbsent` - Must NOT contain placeholders\n\n### Data Flow\n\n```\n~/.config/swarm-tools/\n├── logs/\n│   └── compaction.log      # Structured JSON logs (existing)\n├── sessions/\n│   └── {session_id}.jsonl  # CoordinatorEvent stream (existing)\n```\n\n---\n\n## Proposed Design\n\n### 1. New Event Types\n\nAdd to `CoordinatorEventSchema`:\n\n```typescript\n// COMPACTION events (new)\nz.object({\n  session_id: z.string(),\n  epic_id: z.string().optional(), // May not have epic yet\n  timestamp: z.string(),\n  event_type: z.literal(\"COMPACTION\"),\n  compaction_type: z.enum([\n    \"detection_complete\",\n    \"prompt_generated\", \n    \"context_injected\",\n    \"resumption_verified\"  // Future: track if coordinator resumed correctly\n  ]),\n  payload: z.object({\n    // detection_complete\n    confidence: z.enum([\"high\", \"medium\", \"low\", \"none\"]).optional(),\n    reasons: z.array(z.string()).optional(),\n    session_scan: z.object({\n      message_count: z.number(),\n      tool_call_count: z.number(),\n      high_confidence_count: z.number(),\n      swarm_detected: z.boolean(),\n    }).optional(),\n    \n    // prompt_generated\n    prompt: z.string().optional(),  // FULL prompt, not preview\n    prompt_length: z.number().optional(),\n    model: z.string().optional(),\n    generation_duration_ms: z.number().optional(),\n    \n    // context_injected\n    content: z.string().optional(),  // FULL injected content\n    content_length: z.number().optional(),\n    method: z.enum([\"output.prompt\", \"output.context\"]).optional(),\n    \n    // resumption_verified (future)\n    first_tool_after_compaction: z.string().optional(),\n    resumed_as_coordinator: z.boolean().optional(),\n  }),\n}),\n```\n\n### 2. Capture Points in plugin-wrapper-template.ts\n\n```typescript\n// After detection\ncaptureCoordinatorEvent({\n  session_id: sessionID,\n  epic_id: swarmState.epicId,\n  timestamp: new Date().toISOString(),\n  event_type: \"COMPACTION\",\n  compaction_type: \"detection_complete\",\n  payload: {\n    confidence: detection.confidence,\n    reasons: detection.reasons,\n    session_scan: {\n      message_count: scanResult.messageCount,\n      tool_call_count: scanResult.toolCallCount,\n      high_confidence_count: scanResult.highConfidenceCount,\n      swarm_detected: scanResult.swarmDetected,\n    },\n  },\n});\n\n// After LLM generation\ncaptureCoordinatorEvent({\n  session_id: sessionID,\n  epic_id: swarmState.epicId,\n  timestamp: new Date().toISOString(),\n  event_type: \"COMPACTION\",\n  compaction_type: \"prompt_generated\",\n  payload: {\n    prompt: generatedPrompt,  // FULL prompt\n    prompt_length: generatedPrompt.length,\n    model: \"anthropic/claude-haiku-4-5\",\n    generation_duration_ms: llmDuration,\n  },\n});\n\n// After injection\ncaptureCoordinatorEvent({\n  session_id: sessionID,\n  epic_id: swarmState.epicId,\n  timestamp: new Date().toISOString(),\n  event_type: \"COMPACTION\",\n  compaction_type: \"context_injected\",\n  payload: {\n    content: injectedContent,  // FULL content\n    content_length: injectedContent.length,\n    method: \"output.prompt\",\n  },\n});\n```\n\n### 3. New Scorers\n\n```typescript\n// evals/scorers/compaction-prompt-scorers.ts\n\n/**\n * Prompt contains specific epic ID (not placeholder)\n */\nexport const epicIdSpecificity: Scorer<string, CompactionExpected> = {\n  name: \"epicIdSpecificity\",\n  scorer: async ({ output, expected }) => {\n    const result = JSON.parse(output);\n    const prompt = result.payload?.prompt || \"\";\n    \n    // Must contain actual epic ID pattern, not placeholder\n    const hasRealEpicId = /[a-z]+-lf2p4u-[a-z0-9]+/.test(prompt);\n    const hasPlaceholder = /<epic>|bd-xxx|<cell-id>/.test(prompt);\n    \n    return hasRealEpicId && !hasPlaceholder ? 1 : 0;\n  },\n};\n\n/**\n * Prompt contains actionable tool calls with real values\n */\nexport const actionability: Scorer<string, CompactionExpected> = {\n  name: \"actionability\",\n  scorer: async ({ output, expected }) => {\n    const result = JSON.parse(output);\n    const prompt = result.payload?.prompt || \"\";\n    \n    let score = 0;\n    \n    // Must have swarm_status with real epic_id\n    if (/swarm_status\\(epic_id=\"[^\"<]+\",/.test(prompt)) score += 0.25;\n    \n    // Must have swarmmail_inbox\n    if (/swarmmail_inbox/.test(prompt)) score += 0.25;\n    \n    // Must have project_key with real path\n    if (/project_key=\"\\/[^\"<]+\",/.test(prompt)) score += 0.25;\n    \n    // Must have swarm_spawn_subtask or swarm_review\n    if (/swarm_spawn_subtask|swarm_review/.test(prompt)) score += 0.25;\n    \n    return score;\n  },\n};\n\n/**\n * Coordinator identity reinforcement strength\n */\nexport const coordinatorIdentity: Scorer<string, CompactionExpected> = {\n  name: \"coordinatorIdentity\",\n  scorer: async ({ output }) => {\n    const result = JSON.parse(output);\n    const prompt = result.payload?.prompt || \"\";\n    \n    let score = 0;\n    \n    // ASCII header present\n    if (/[┌─┐│└┘╔═╗║╚╝]/.test(prompt)) score += 0.2;\n    \n    // \"YOU ARE THE COORDINATOR\" appears 2+ times\n    const matches = prompt.match(/YOU ARE THE COORDINATOR/gi) || [];\n    if (matches.length >= 2) score += 0.3;\n    \n    // NEVER/ALWAYS/NON-NEGOTIABLE language\n    if (/NEVER/.test(prompt)) score += 0.1;\n    if (/ALWAYS/.test(prompt)) score += 0.1;\n    if (/NON-NEGOTIABLE/.test(prompt)) score += 0.1;\n    \n    // \"NOT A WORKER\" or \"DO NOT IMPLEMENT\"\n    if (/NOT A WORKER|DO NOT IMPLEMENT/i.test(prompt)) score += 0.2;\n    \n    return score;\n  },\n};\n\n/**\n * Forbidden tools section present\n */\nexport const forbiddenToolsPresent: Scorer<string, CompactionExpected> = {\n  name: \"forbiddenToolsPresent\",\n  scorer: async ({ output }) => {\n    const result = JSON.parse(output);\n    const prompt = result.payload?.prompt || \"\";\n    \n    const forbiddenTools = [\n      \"repo-crawl\", \"repo-autopsy\", \"webfetch\", \n      \"fetch_fetch\", \"context7\", \"pdf-brain\"\n    ];\n    \n    const foundCount = forbiddenTools.filter(t => prompt.includes(t)).length;\n    return foundCount / forbiddenTools.length;\n  },\n};\n```\n\n### 4. New Eval File\n\n```typescript\n// evals/compaction-prompt.eval.ts\n\nimport { evalite } from \"evalite\";\nimport { loadCompactionEvents } from \"./lib/compaction-loader.js\";\nimport {\n  epicIdSpecificity,\n  actionability,\n  coordinatorIdentity,\n  forbiddenToolsPresent,\n} from \"./scorers/compaction-prompt-scorers.js\";\n\nevalite(\"Compaction Prompt Quality\", {\n  data: async () => {\n    // Load real compaction events from sessions\n    const events = await loadCompactionEvents({ \n      type: \"prompt_generated\",\n      limit: 50 \n    });\n    \n    return events.map(event => ({\n      input: event,\n      expected: {\n        hasRealEpicId: true,\n        hasActionableTools: true,\n        hasStrongIdentity: true,\n        hasForbiddenTools: true,\n      },\n    }));\n  },\n\n  task: async (input) => JSON.stringify(input),\n\n  scorers: [\n    epicIdSpecificity,\n    actionability,\n    coordinatorIdentity,\n    forbiddenToolsPresent,\n  ],\n});\n```\n\n### 5. Data Loader\n\n```typescript\n// evals/lib/compaction-loader.ts\n\nimport * as fs from \"node:fs\";\nimport * as path from \"node:path\";\nimport * as os from \"node:os\";\n\nexport interface CompactionEvent {\n  session_id: string;\n  epic_id?: string;\n  timestamp: string;\n  event_type: \"COMPACTION\";\n  compaction_type: string;\n  payload: Record<string, unknown>;\n}\n\nexport async function loadCompactionEvents(options?: {\n  type?: \"detection_complete\" | \"prompt_generated\" | \"context_injected\";\n  limit?: number;\n  sessionIds?: string[];\n}): Promise<CompactionEvent[]> {\n  const sessionDir = path.join(os.homedir(), \".config\", \"swarm-tools\", \"sessions\");\n  \n  if (!fs.existsSync(sessionDir)) {\n    return [];\n  }\n  \n  const files = fs.readdirSync(sessionDir).filter(f => f.endsWith(\".jsonl\"));\n  const events: CompactionEvent[] = [];\n  \n  for (const file of files) {\n    if (options?.sessionIds && !options.sessionIds.includes(file.replace(\".jsonl\", \"\"))) {\n      continue;\n    }\n    \n    const content = fs.readFileSync(path.join(sessionDir, file), \"utf-8\");\n    const lines = content.trim().split(\"\\n\").filter(Boolean);\n    \n    for (const line of lines) {\n      try {\n        const event = JSON.parse(line);\n        if (event.event_type === \"COMPACTION\") {\n          if (!options?.type || event.compaction_type === options.type) {\n            events.push(event);\n          }\n        }\n      } catch {\n        // Skip invalid lines\n      }\n    }\n    \n    if (options?.limit && events.length >= options.limit) {\n      break;\n    }\n  }\n  \n  return events.slice(0, options?.limit);\n}\n```\n\n---\n\n## Implementation Plan\n\n### Phase 1: Schema & Capture (src/eval-capture.ts)\n1. Add COMPACTION event type to CoordinatorEventSchema\n2. Export captureCompactionEvent helper function\n3. Add tests for new schema\n\n### Phase 2: Hook Integration (plugin-wrapper-template.ts)\n1. Import captureCompactionEvent\n2. Add capture calls at detection, generation, injection points\n3. Capture FULL prompt content (not just preview)\n\n### Phase 3: Scorers (evals/scorers/compaction-prompt-scorers.ts)\n1. epicIdSpecificity\n2. actionability\n3. coordinatorIdentity\n4. forbiddenToolsPresent\n5. Add tests for each scorer\n\n### Phase 4: Eval & Loader (evals/)\n1. compaction-loader.ts - Load events from sessions\n2. compaction-prompt.eval.ts - Main eval file\n3. Add to eval:run script\n\n### Phase 5: Verification\n1. Run compaction, verify events captured\n2. Run eval, verify scores make sense\n3. Document in evals/README.md\n\n---\n\n## Files to Modify\n\n| File | Changes |\n|------|---------|\n| `src/eval-capture.ts` | Add COMPACTION event type, captureCompactionEvent |\n| `examples/plugin-wrapper-template.ts` | Add capture calls at 3 points |\n| `evals/scorers/compaction-prompt-scorers.ts` | New file with 4 scorers |\n| `evals/lib/compaction-loader.ts` | New file to load events |\n| `evals/compaction-prompt.eval.ts` | New eval file |\n| `evals/README.md` | Document new eval |\n\n---\n\n## Success Criteria\n\n1. **Capture works**: After compaction, `~/.config/swarm-tools/sessions/{session}.jsonl` contains COMPACTION events with full prompt content\n2. **Scorers work**: `bun run eval:run` includes compaction-prompt eval with meaningful scores\n3. **Visibility**: Can query \"what prompts were generated in the last 24h\" and score them\n4. **Regression detection**: If prompt quality degrades, eval scores drop","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-25T03:34:23.548Z","updated_at":"2025-12-25T03:38:24.655Z","closed_at":"2025-12-25T03:38:24.655Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81rzt4z","title":"Add COMPACTION event schema to eval-capture.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:38:41.279Z","updated_at":"2025-12-25T03:42:22.346Z","closed_at":"2025-12-25T03:42:22.346Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81s2tl7","title":"Add compaction event capture to plugin-wrapper-template.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:38:41.282Z","updated_at":"2025-12-25T03:42:23.072Z","closed_at":"2025-12-25T03:42:23.072Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81s6a7u","title":"Add post-compaction tool call tracking hook","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:38:41.286Z","updated_at":"2025-12-25T03:42:23.809Z","closed_at":"2025-12-25T03:42:23.809Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81s91zk","title":"Create all compaction prompt quality scorers","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:38:41.289Z","updated_at":"2025-12-25T03:42:24.542Z","closed_at":"2025-12-25T03:42:24.542Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81sa139","title":"Create compaction data loader","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:38:41.290Z","updated_at":"2025-12-25T03:42:25.282Z","closed_at":"2025-12-25T03:42:25.282Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81sd0ys","title":"Create compaction-prompt.eval.ts and fixtures","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:38:41.293Z","updated_at":"2025-12-25T03:42:26.021Z","closed_at":"2025-12-25T03:42:26.021Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81sfatk","title":"Update evals/README.md with compaction eval docs","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:38:41.295Z","updated_at":"2025-12-25T03:42:26.674Z","closed_at":"2025-12-25T03:42:26.674Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkw81rkq4c","title":"Compaction Eval System with Full Reaction Tracking","description":"**Eval-Driven Development with Progressive Gates**\n\nBake evals into the soul of the project. Every change gets scored, regressions are impossible to ignore, the system learns from itself.\n\n---\n\n# ADR: Eval-Driven Development System\n\n## Status\n**Proposed** - Ready for implementation\n\n## Context\n\nThe opencode-swarm-plugin generates prompts for coordinator continuation after compaction. Currently:\n- Compaction logs exist but only capture previews (first 500 chars)\n- No structured capture for eval consumption\n- No scoring of prompt quality\n- No tracking of post-compaction agent behavior\n- No CI gates on quality\n- No learning loop from failures\n\nWe want evals to be the **forcing function for quality** - not an afterthought.\n\n## Decision\n\nImplement a complete eval-driven development system with progressive gates.\n\n### Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                         EVAL PIPELINE                           │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  CAPTURE → SCORE → STORE → GATE → LEARN → IMPROVE              │\n│                                                                 │\n│  ┌─────────┐   ┌─────────┐   ┌─────────┐   ┌─────────┐         │\n│  │ Events  │──▶│ Scorers │──▶│ History │──▶│  Gates  │         │\n│  │ JSONL   │   │ Evalite │   │ JSONL   │   │   CI    │         │\n│  └─────────┘   └─────────┘   └─────────┘   └────┬────┘         │\n│       │                            │            │               │\n│       │                            ▼            ▼               │\n│       │                      ┌─────────┐   ┌─────────┐         │\n│       │                      │ Learning│──▶│ Improve │         │\n│       │                      │ Memory  │   │ Prompts │         │\n│       │                      └─────────┘   └────┬────┘         │\n│       │                                         │               │\n│       └─────────────────────────────────────────┘               │\n│                      (closed loop)                              │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### Progressive Gate Phases\n\n| Phase | Runs | Variance | Behavior |\n|-------|------|----------|----------|\n| **Bootstrap** | < 10 | any | No gates, collect data only |\n| **Stabilization** | 10-50 | any | Soft gates, warn on >10% regression |\n| **Production** | > 50 | < 0.1 | Hard gates, fail on >5% regression |\n\n### Event Types (COMPACTION)\n\n```typescript\ntype CompactionEventType = \n  | \"detection_complete\"    // Swarm detected, confidence, reasons\n  | \"prompt_generated\"      // FULL prompt content, model, duration\n  | \"context_injected\"      // FULL injected content, method\n  | \"resumption_started\"    // First tool call after compaction\n  | \"tool_call_tracked\"     // Each tool call (first 20)\n```\n\n### Scorers\n\n| Scorer | What It Measures | Weight |\n|--------|------------------|--------|\n| `epicIdSpecificity` | Real IDs, not placeholders | 0.20 |\n| `actionability` | swarm_status/inbox with real values | 0.20 |\n| `coordinatorIdentity` | ASCII header, NEVER/ALWAYS, repeated statements | 0.25 |\n| `forbiddenToolsPresent` | Lists all forbidden tools by name | 0.15 |\n| `postCompactionDiscipline` | First tool correct, no edit/write, spawns workers | 0.20 |\n\n### Learning Loop\n\nWhen eval score drops significantly:\n1. Extract failure context (what prompt, what score, what went wrong)\n2. Store to semantic-memory with tags: `eval-failure`, `compaction`, `<scorer-name>`\n3. Future prompt generation queries memory for past failures\n4. Prompts improve, scores rise, gates tighten\n\n### CLI Commands\n\n```bash\nswarm eval status    # Current phase, thresholds, recent scores\nswarm eval history   # Score history with sparklines\nswarm eval run       # Run evals, report results\n```\n\n### CI Integration\n\n```yaml\n# .github/workflows/ci.yml\n- name: Run Evals\n  run: bun run eval:ci\n  # Bootstrap: always passes, reports scores\n  # Stabilization: warns on regression\n  # Production: fails on regression\n```\n\n## Consequences\n\n### Positive\n- Quality is enforced, not hoped for\n- Regressions are caught automatically\n- System learns from failures\n- Progressive gates prevent blocking during early development\n- Full visibility into prompt quality over time\n\n### Negative\n- More infrastructure to maintain\n- Eval runs add CI time (~30s)\n- Need to bootstrap with enough data before gates activate\n\n### Neutral\n- Developers must understand eval system\n- New evals require scorer + fixture + documentation\n\n## Implementation\n\n12 subtasks, ~35 complexity points, parallelizable in 4 waves:\n\n**Wave 1 (no deps):** Schema, Scorers, History tracker\n**Wave 2 (after Wave 1):** Capture wiring, Data loader, Gates, Learning\n**Wave 3 (after Wave 2):** Post-compaction tracker, Eval file, CLI\n**Wave 4 (after Wave 3):** CI workflow, Documentation\n\n---\n\n## Files\n\n### New Files\n- `src/eval-capture.ts` - COMPACTION event schema (extend existing)\n- `src/post-compaction-tracker.ts` - Tool call tracking hook\n- `src/eval-history.ts` - Score history storage\n- `src/eval-gates.ts` - Progressive gate logic\n- `src/eval-learning.ts` - Failure-to-memory loop\n- `evals/scorers/compaction-prompt-scorers.ts` - 5 new scorers\n- `evals/lib/compaction-loader.ts` - Load COMPACTION events\n- `evals/compaction-prompt.eval.ts` - Main eval file\n- `evals/fixtures/compaction-prompt-cases.ts` - Synthetic test cases\n\n### Modified Files\n- `examples/plugin-wrapper-template.ts` - Add capture calls\n- `bin/swarm.ts` - Add eval CLI commands\n- `.github/workflows/ci.yml` - Add eval step\n- `package.json` - Add eval:ci script\n- `evals/README.md` - Full system documentation\n\n---\n\n## Success Criteria\n\n1. **Capture works**: After compaction, `sessions/*.jsonl` contains COMPACTION events with full prompt\n2. **Scores are meaningful**: Scorers detect real quality differences\n3. **History tracks**: `eval-history.jsonl` grows with each run\n4. **Gates progress**: System moves from bootstrap → stabilization → production\n5. **Learning works**: Low scores create semantic-memory entries\n6. **CI integrates**: PRs show eval scores, production phase blocks regressions\n7. **CLI is useful**: `swarm eval status` shows actionable information\n8. **Docs are complete**: README explains everything needed to use and extend","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-25T03:38:41.264Z","updated_at":"2025-12-25T03:43:48.564Z","closed_at":"2025-12-25T03:43:48.564Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehsyarj","title":"[TDD] Add COMPACTION event schema and capture helpers","description":"RED: Write failing tests for COMPACTION event schema - test each compaction_type (detection_complete, prompt_generated, context_injected, resumption_started, tool_call_tracked), test payload validation, test captureCompactionEvent() writes to correct path. GREEN: Add COMPACTION to CoordinatorEventSchema discriminated union, implement typed payloads, export helper. REFACTOR: Clean up, ensure consistent with existing event patterns.\n\nFiles: eval-capture.ts, eval-capture.test.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:43:41.986Z","updated_at":"2025-12-25T04:05:12.773Z","closed_at":"2025-12-25T04:05:12.773Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkweht12x0","title":"[TDD] Create eval score history tracker","description":"RED: Write failing tests for recordEvalRun(), getScoreHistory(), calculateVariance(), getPhase(). Test phase transitions: <10 runs = bootstrap, 10-50 = stabilization, >50 + low variance = production. Test variance calculation. Test JSONL append/read. GREEN: Implement eval-history.ts with all functions. REFACTOR: Extract constants, ensure atomic writes.\n\nFiles: eval-history.ts, eval-history.test.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:43:41.989Z","updated_at":"2025-12-25T04:05:14.736Z","closed_at":"2025-12-25T04:05:14.736Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkweht4ocy","title":"[TDD] Create compaction prompt quality scorers","description":"RED: Write failing tests for each scorer with known inputs/outputs. Test epicIdSpecificity with real ID vs placeholder. Test actionability with/without swarm_status. Test coordinatorIdentity with/without ASCII header. Test forbiddenToolsPresent with partial/full lists. Test postCompactionDiscipline with good/bad tool sequences. GREEN: Implement all 5 scorers. REFACTOR: Extract shared regex patterns, add JSDoc.\n\nFiles: evals/scorers/compaction-prompt-scorers.ts, evals/scorers/compaction-prompt-scorers.test.ts","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T03:43:41.992Z","updated_at":"2025-12-25T04:05:16.770Z","closed_at":"2025-12-25T04:05:16.770Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkweht69lv","title":"[TDD] Create progressive gate logic","description":"RED: Write failing tests for checkGate() in each phase. Bootstrap: always passes. Stabilization: passes unless >10% regression. Production: fails on >5% regression. Test edge cases: exactly 10 runs, exactly 50 runs, variance at 0.1 boundary. GREEN: Implement eval-gates.ts. REFACTOR: Make thresholds configurable, add clear error messages.\n\nDepends on: mjkweht12x0 (history tracker)\nFiles: eval-gates.ts, eval-gates.test.ts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:41.994Z","updated_at":"2025-12-25T04:13:50.775Z","closed_at":"2025-12-25T04:13:50.775Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkweht7320","title":"[TDD] Create eval-to-learning feedback loop","description":"RED: Write failing tests for learnFromEvalFailure(). Test that significant drops trigger memory storage. Test memory content includes eval name, score, context. Test that minor fluctuations don't trigger. Mock semantic-memory calls. GREEN: Implement eval-learning.ts. REFACTOR: Add configurable threshold for 'significant drop'.\n\nDepends on: mjkweht12x0 (history tracker)\nFiles: eval-learning.ts, eval-learning.test.ts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:41.995Z","updated_at":"2025-12-25T04:13:52.394Z","closed_at":"2025-12-25T04:13:52.394Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkweht9dn4","title":"[TDD] Create compaction data loader","description":"RED: Write failing tests for loadCompactionEvents() and loadCompactionSessions(). Test filtering by type, limit, sessionIds. Test handling of missing directory. Test grouping by session. Use fixture JSONL files. GREEN: Implement compaction-loader.ts. REFACTOR: Add streaming for large files, improve error handling.\n\nDepends on: mjkwehsyarj (schema)\nFiles: evals/lib/compaction-loader.ts, evals/lib/compaction-loader.test.ts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:41.997Z","updated_at":"2025-12-25T04:13:53.601Z","closed_at":"2025-12-25T04:13:53.601Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehtburk","title":"[TDD] Create post-compaction tool call tracker","description":"RED: Write failing tests for tracker. Test resumption_started emitted on first tool. Test tool_call_tracked for each of first 20 calls. Test is_coordinator_violation detection (edit, write, reserve). Test tracking stops after 20 calls. GREEN: Implement post-compaction-tracker.ts with tool.call hook. REFACTOR: Make call limit configurable.\n\nDepends on: mjkwehsyarj (schema)\nFiles: post-compaction-tracker.ts, post-compaction-tracker.test.ts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:41.999Z","updated_at":"2025-12-25T04:13:55.015Z","closed_at":"2025-12-25T04:13:55.015Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehtdoxz","title":"Wire compaction capture into plugin-wrapper-template.ts","description":"Integration work - no unit tests, but verify with integration test. Import captureCompactionEvent. Add capture at 3 points: (1) after detection with confidence/reasons/session_scan, (2) after LLM generation with FULL prompt, (3) after injection with FULL content. Verify events appear in session JSONL after running compaction.\n\nDepends on: mjkwehsyarj (schema), mjkwehtburk (tracker)\nFiles: examples/plugin-wrapper-template.ts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:42.001Z","updated_at":"2025-12-25T04:25:06.777Z","closed_at":"2025-12-25T04:25:06.777Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehtfhxs","title":"[TDD] Create compaction-prompt.eval.ts with fixtures","description":"RED: Write test that eval runs without error, produces scores, records to history. Test fallback to fixtures when no real data. Test composite scorer weights correctly. GREEN: Create eval file using Evalite, wire all scorers, add history recording, add gate checking. Create fixtures/compaction-prompt-cases.ts with synthetic cases. REFACTOR: Add eval:compaction script to package.json.\n\nDepends on: mjkweht4ocy (scorers), mjkweht69lv (gates), mjkweht7320 (learning), mjkweht9dn4 (loader)\nFiles: evals/compaction-prompt.eval.ts, evals/fixtures/compaction-prompt-cases.ts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:42.003Z","updated_at":"2025-12-25T04:25:07.991Z","closed_at":"2025-12-25T04:25:07.991Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehth7o8","title":"[TDD] Add swarm eval CLI commands","description":"RED: Write failing tests for CLI commands. Test 'eval status' outputs phase, thresholds, scores. Test 'eval history' shows entries with formatting. Test 'eval run' executes and reports. GREEN: Add commands to bin/swarm.ts using existing CLI patterns. REFACTOR: Add colors, sparklines for trends, clear formatting.\n\nDepends on: mjkweht12x0 (history), mjkweht69lv (gates)\nFiles: bin/swarm.ts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:42.005Z","updated_at":"2025-12-25T04:25:08.910Z","closed_at":"2025-12-25T04:25:08.910Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehtj1t6","title":"Add CI workflow for eval gates","description":"Integration/config work. Update .github/workflows/ci.yml to run evals after tests. Add eval:ci script that runs evals with --ci flag (uses checkGate, exits non-zero on production failure). Add step to post scores as PR comment. Test locally with act if possible.\n\nDepends on: mjkweht69lv (gates), mjkwehtfhxs (eval file)\nFiles: .github/workflows/ci.yml, package.json","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T03:43:42.007Z","updated_at":"2025-12-25T04:33:18.544Z","closed_at":"2025-12-25T04:33:18.544Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehtlf3p","title":"Update all documentation for eval system","description":"Documentation is part of the story. Update evals/README.md with: architecture diagram, progressive gates explanation, scorer reference, CLI commands, CI integration, how to add new evals, troubleshooting. Update main README.md with eval system overview. Add inline JSDoc to all new exports. Ensure examples are runnable.\n\nDepends on: mjkwehtfhxs (eval file), mjkwehth7o8 (CLI), mjkwehtj1t6 (CI)\nFiles: evals/README.md, README.md","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-25T03:43:42.009Z","updated_at":"2025-12-25T04:33:19.672Z","closed_at":"2025-12-25T04:33:19.672Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkx3uph7h4","title":"Migrate from bun:test to vitest for unified test runner","description":"Running two test runners (bun:test + vitest for evalite) is a PITA. Vitest is dope and evalite requires it anyway.\n\n**Current state:**\n- src/ tests use bun:test\n- evals/ use evalite which requires vitest\n- bunfig.toml excludes evals/ from bun test discovery\n- Importing evalite scorers in bun:test causes \"inject not found\" errors\n\n**Migration:**\n1. Replace all `import { describe, test, expect } from \"bun:test\"` with vitest\n2. Update package.json test scripts to use vitest\n3. Remove bunfig.toml test config (vitest has its own)\n4. Add vitest.config.ts\n5. Verify all tests pass\n\n**Benefits:**\n- Single test runner\n- Better watch mode\n- Native evalite compatibility\n- Consistent API across all tests","status":"open","priority":2,"issue_type":"chore","created_at":"2025-12-25T04:03:25.109Z","updated_at":"2025-12-25T04:03:25.109Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk6mha6mi2","title":"Add observability to pre-compaction hook","description":"**Problem:** The pre-compaction hook does significant work (context analysis, memory extraction, pattern detection) but provides zero visibility into what it's doing or how well it's working.\n\n**Current state:**\n- Hook runs silently before context compaction\n- No logging of what patterns were detected\n- No metrics on extraction success/failure\n- No way to debug why certain learnings weren't captured\n- Can't analyze effectiveness over time\n\n**What we need:**\n\n1. **Structured logging** - What the hook analyzed, what it extracted, what it skipped\n2. **Metrics** - Success rates, extraction counts, timing data\n3. **Debug mode** - Verbose output showing decision process\n4. **Post-hoc analysis** - Queryable history of hook runs\n\n**Potential approaches:**\n- Emit events to swarm-mail event store (queryable, persistent)\n- Write to `.hive/compaction-log.jsonl` (simple, greppable)\n- Add `--verbose` flag to hook invocation\n- Dashboard/CLI command to analyze hook effectiveness\n\n**Files:**\n- `packages/opencode-swarm-plugin/src/compaction-hook.ts`\n- Possibly new: `src/compaction-observability.ts`\n\n**Success criteria:**\n- Can answer: \"What did the last 10 compaction runs extract?\"\n- Can answer: \"Why didn't this pattern get captured?\"\n- Can see timing breakdown (analysis vs extraction vs storage)","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-24T15:42:04.542Z","updated_at":"2025-12-25T05:32:02.450Z","closed_at":"2025-12-25T05:32:02.450Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjk8tk7jn11","title":"Research: Coordinator prompt iteration with evals + o11y feedback loop","description":"## Problem\n\nThe coordinator prompt in `bin/swarm.ts` is ~500 lines of instructions that we iterate on by feel. When something goes wrong (like me skipping decomposition validation), we add more warnings. But we have no way to:\n\n1. **Measure** if the prompt changes actually work\n2. **A/B test** different prompt versions\n3. **Correlate** prompt changes with swarm outcomes\n4. **Identify** which instructions are ignored vs followed\n\n## What We Have Now\n\n- **Observability tools** (just committed): analytics queries, event store, pre-built queries\n- **Learning system**: pattern maturity, confidence decay, outcome tracking\n- **Swarm events**: task_started, task_completed, task_blocked, review_approved, review_rejected\n\n## What We Need\n\n### Option A: Lightweight - Prompt Versioning + Analytics\n\n1. Version the coordinator prompt (hash or semver)\n2. Tag swarm events with prompt version\n3. Query: \"success rate by prompt version\"\n4. Manual iteration based on data\n\n### Option B: Medium - Evalite Integration\n\nWe have `evals/` directory with evalite setup. Could:\n1. Create coordinator prompt evals (given task, does it decompose correctly?)\n2. Test prompt variants offline before deploying\n3. Score: file conflicts detected, validation used, retry loop followed\n\n### Option C: Full - LLM-as-Judge Continuous Eval\n\n1. After each swarm completes, run eval on coordinator behavior\n2. LLM judges: \"Did coordinator follow the prompt?\"\n3. Auto-flag prompt sections that are consistently ignored\n4. Suggest prompt improvements\n\n## Research Questions\n\n1. What's the minimum viable eval for coordinator prompts?\n2. Can we use our existing o11y data to measure prompt effectiveness?\n3. How do we handle the meta-problem (prompt for evaluating prompts)?\n4. What does evalite need to test coordinator behavior specifically?\n\n## Related\n\n- `.hive/analysis/observability-*.md` - ADRs on o11y\n- `evals/` directory - existing evalite setup\n- `swarm-decomposition.eval.ts` - existing decomposition eval\n- OpenCode issue #5887 - async agents (future capability)\n\n## Deliverables\n\n1. Analysis of options A/B/C with effort estimates\n2. Recommendation for MVP approach\n3. If Option B: spec for coordinator prompt evals\n4. Integration plan with existing o11y tools","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T16:43:34.159Z","updated_at":"2025-12-25T05:32:04.102Z","closed_at":"2025-12-25T05:32:04.102Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkiu8kz2yi","title":"Add 'swarm log sessions' subcommand for coordinator events","description":"The coordinator observability epic added session capture to ~/.config/swarm-tools/sessions/{session_id}.jsonl but `swarm log` only reads from ~/.config/swarm-tools/logs/. \n\nNeed to add:\n- `swarm log sessions` - list all captured sessions\n- `swarm log sessions <session_id>` - show events for a session\n- `swarm log sessions --latest` - show most recent session\n- Support for --json, --level, --since filters\n\nThis completes the observability story - capture + view + score.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-24T21:24:01.907Z","updated_at":"2025-12-25T05:32:05.309Z","closed_at":"2025-12-25T05:32:05.309Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkiubtwumm","title":"Document coordinator observability in README","description":"The coordinator observability epic added:\n- CoordinatorEvent schemas (DECISION, VIOLATION, OUTCOME)\n- Session capture to ~/.config/swarm-tools/sessions/\n- Violation detection in planning-guardrails.ts\n- Decision tracing in swarm tools\n- coordinator-discipline scorers for evalite\n- coordinator-session.eval.ts\n\nNone of this is documented in README.md or AGENTS.md. Need to add:\n1. How session capture works\n2. How to run the eval\n3. What the scorers measure\n4. How to view captured sessions (once swarm log sessions exists)","status":"closed","priority":1,"issue_type":"chore","created_at":"2025-12-24T21:24:06.116Z","updated_at":"2025-12-25T05:32:06.591Z","closed_at":"2025-12-25T05:32:06.591Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkwehsqnbm","title":"Eval-Driven Development with Progressive Gates","description":"Bake evals into the soul of the project. TDD everything. Every change gets scored, regressions are impossible to ignore, the system learns from itself. Pipeline: CAPTURE → SCORE → STORE → GATE → LEARN → IMPROVE","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-25T03:43:41.978Z","updated_at":"2025-12-25T05:18:38.518Z","closed_at":"2025-12-25T05:18:38.518Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjkzuy0j0kn","title":"hive_query should support parent_id filter for finding children","description":"**Problem:** `hive_query` and `hive_cells` don't support filtering by `parent_id`, making it impossible to find children of an epic without rawdogging the JSONL.\n\n**Current behavior:**\n```typescript\nhive_query({ status: \"open\" })  // works\nhive_cells({ status: \"open\" })  // works\nhive_cells({ parent_id: \"mjhk4kkh975\" })  // NOT SUPPORTED\n```\n\n**Expected:**\n```typescript\nhive_cells({ parent_id: \"mjhk4kkh975\" })  // returns all children of epic\n```\n\n**Impact:**\n- Can't easily query \"what's open under this epic?\"\n- Forces grep/jq on JSONL files\n- Breaks the \"use tools not raw files\" principle\n\n**Fix locations:**\n- `packages/swarm-mail/src/hive/adapter.ts` - `queryCells()` method\n- `packages/opencode-swarm-plugin/src/hive.ts` - `hive_cells` tool schema\n- Add `parent_id` to the query filter options\n\n**TDD approach:**\n1. Add failing test: `queryCells({ parent_id: \"epic-123\" })` returns children\n2. Update adapter to support parent_id filter\n3. Update tool schema to expose the parameter","status":"open","priority":1,"issue_type":"bug","created_at":"2025-12-25T05:20:28.339Z","updated_at":"2025-12-25T05:20:28.339Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl0n8rl28k","title":"Eval-Driven Coordinator Template Rewrite","description":"Rewrite swarm.md command template based on actual eval data and add missing event tracking.\n\n**Goals:**\n- Template structured around what evals measure\n- Clear pass/fail criteria for coordinators\n- Researcher spawning as first-class citizen\n- Less prose, more checklists\n- Add missing eval events (researcher_spawned, skill_loaded, inbox_checked)","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-25T05:42:28.641Z","updated_at":"2025-12-25T05:59:27.822Z","closed_at":"2025-12-25T05:59:27.822Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl0n8rv0th","title":"Add missing coordinator event types to eval capture","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T05:42:28.651Z","updated_at":"2025-12-25T05:59:20.283Z","closed_at":"2025-12-25T05:59:20.283Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl0n8rl28k","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl0n8rylpp","title":"Rewrite swarm.md command template - eval-driven structure","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T05:42:28.654Z","updated_at":"2025-12-25T05:59:22.107Z","closed_at":"2025-12-25T05:59:22.107Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl0n8rl28k","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","title":"Mem0/A-MEM Inspired Memory System Overhaul","description":"Full implementation of Mem0/A-MEM memory patterns:\n\n1. **Smart Memory Operations** - LLM-driven ADD/UPDATE/DELETE/NOOP decisions\n2. **Auto-tagging** - Generate tags/keywords on store for better retrieval\n3. **Memory Linking** - Bidirectional Zettelkasten-style connections\n4. **Proactive Extraction** - Extract learnings after exchanges, not just on compaction\n5. **Entity/Relationship Extraction** - Graph-style triples for multi-hop reasoning\n6. **Temporal Validity** - valid_from/valid_until windows with supersession\n\nKey insight from Mem0 paper: \"Memory is essential for communication: we recall past interactions, infer preferences, and construct evolving mental models.\"\n\nOur gap: Current system just appends. Mem0/A-MEM treat memory as a living graph that evolves.","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-25T06:08:33.651Z","updated_at":"2025-12-25T06:08:33.651Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1kscdgxg","title":"Schema: Add memory_links, entities, relationships tables","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T06:08:33.661Z","updated_at":"2025-12-25T06:34:58.577Z","closed_at":"2025-12-25T06:34:58.577Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1kscjw3s","title":"Core: Implement LLM-driven memory operations (ADD/UPDATE/DELETE/NOOP)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T06:08:33.667Z","updated_at":"2025-12-25T06:35:00.827Z","closed_at":"2025-12-25T06:35:00.827Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1kscpadl","title":"Core: Implement auto-tagging service using LLM","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T06:08:33.673Z","updated_at":"2025-12-25T06:35:02.789Z","closed_at":"2025-12-25T06:35:02.789Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1ksch3wb","title":"Schema: Add temporal fields (valid_from, valid_until, superseded_by)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-25T06:08:33.665Z","updated_at":"2025-12-25T14:20:12.129Z","closed_at":"2025-12-25T14:20:12.129Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1kscsxga","title":"Core: Implement memory linking (find related, create bidirectional links)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T06:08:33.676Z","updated_at":"2025-12-25T14:47:11.706Z","closed_at":"2025-12-25T14:47:11.706Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1kscu5bq","title":"Core: Implement entity/relationship extraction","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T06:08:33.678Z","updated_at":"2025-12-25T14:47:14.219Z","closed_at":"2025-12-25T14:47:14.219Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1kscwp3n","title":"Adapter: Update MemoryAdapter with upsert, temporal queries, graph queries","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T06:08:33.680Z","updated_at":"2025-12-25T14:47:17.050Z","closed_at":"2025-12-25T14:47:17.050Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1ksd045f","title":"Plugin: Add semantic-memory_upsert tool with smart operations","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T06:08:33.684Z","updated_at":"2025-12-25T14:47:18.800Z","closed_at":"2025-12-25T14:47:18.800Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1ksd30wf","title":"Plugin: Add proactive extraction hook (post-exchange memory extraction)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-25T06:08:33.687Z","updated_at":"2025-12-25T14:59:09.073Z","closed_at":"2025-12-25T14:59:09.073Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjl1ksd6fgx","title":"Integration: Wire up all components, update exports, add integration tests","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-25T06:08:33.690Z","updated_at":"2025-12-25T15:16:25.513Z","closed_at":"2025-12-25T15:16:25.513Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjll5h622kf","title":"Chore: Update adapter.test.ts for graceful degradation behavior","description":"3 unit tests in adapter.test.ts fail after integration wiring (mjl1ksd6fgx) because they expect old stub behavior:\n\n1. `upsert() with refined information returns UPDATE` - expects UPDATE but gets ADD (fallback heuristics)\n2. `upsert() with contradicting information returns DELETE` - expects DELETE but gets ADD (fallback heuristics)  \n3. `store() with autoTag extracts tags from content` - expects autoTags but gets undefined (graceful degradation)\n\nThese tests were written for stub implementations that had specific heuristics. Now with real services wired (that fail gracefully without API key), the behavior is different.\n\n**Fix options:**\n1. Update tests to expect graceful degradation behavior (ADD/undefined when no API key)\n2. Mock the LLM services in tests to return expected values\n3. Skip tests that require API key with `.skipIf(!process.env.AI_GATEWAY_API_KEY)`\n\nRecommended: Option 3 - skip tests that require real LLM, keep integration tests for graceful degradation.","status":"open","priority":2,"issue_type":"chore","created_at":"2025-12-25T15:16:31.658Z","updated_at":"2025-12-25T15:16:31.658Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm3nun4fn","title":"Docs: Update swarm-mail README and docs for Wave 1-3 memory features","description":"The swarm-mail README and documentation need updating after Wave 1-3 memory system overhaul.\n\n## Current Issues\n\n1. **README.md** mentions \"pgvector\" but we use libSQL with native vector support\n2. **README.md** Semantic Memory section is minimal - no mention of new features\n3. **INTEGRATION.md** is auto-tagger specific, needs broader coverage\n4. No documentation for new features:\n   - Smart upsert (ADD/UPDATE/DELETE/NOOP)\n   - Auto-tagging with LLM\n   - Memory linking (Zettelkasten-style)\n   - Entity extraction (knowledge graph)\n   - Temporal queries (valid_from/valid_until)\n   - Graph queries (findByEntity, getKnowledgeGraph)\n\n## Files to Update\n\n- `packages/swarm-mail/README.md` - Add Wave 1-3 features section\n- `packages/swarm-mail/src/memory/INTEGRATION.md` - Expand to cover all services\n- Consider: `packages/swarm-mail/src/memory/README.md` - Dedicated memory docs\n\n## Content to Add\n\n### README.md - Semantic Memory Section\n\n```markdown\n## Semantic Memory (Wave 1-3)\n\nVector embeddings + knowledge graph for persistent agent learnings.\n\n### Basic Usage\n```typescript\nconst adapter = createMemoryAdapter(db, config);\n\n// Simple store (backward compatible)\nawait adapter.store(\"OAuth needs refresh buffer\");\n\n// Smart store with auto-features\nawait adapter.store(\"Joel prefers TypeScript\", {\n  autoTag: true,        // LLM extracts tags\n  autoLink: true,       // Links to related memories\n  extractEntities: true // Builds knowledge graph\n});\n\n// Smart upsert (Mem0 pattern)\nconst result = await adapter.upsert(\"OAuth needs 5min buffer\", {\n  useSmartOps: true  // LLM decides: ADD, UPDATE, DELETE, or NOOP\n});\nconsole.log(result.operation); // \"UPDATE\" - refined existing memory\n```\n\n### Knowledge Graph\n```typescript\n// Find memories by entity\nconst joelMemories = await adapter.findByEntity(\"Joel\", \"person\");\n\n// Get knowledge graph for a memory\nconst graph = await adapter.getKnowledgeGraph(memoryId);\n// { entities: [...], relationships: [...] }\n```\n\n### Temporal Queries\n```typescript\n// Find memories valid at specific time\nconst pastMemories = await adapter.findValidAt(\"auth\", new Date(\"2024-01-01\"));\n\n// Track supersession chain\nconst chain = await adapter.getSupersessionChain(memoryId);\n```\n```\n\n## New Exports to Document\n\nFrom index.ts:\n- `analyzeMemoryOperation` - Smart ops service\n- `generateTags` - Auto-tagging service\n- `autoLinkMemory`, `createLink`, `findRelatedMemories` - Linking service\n- `extractEntitiesAndRelationships`, `storeEntities` - Entity extraction\n\n## Schema to Document\n\nNew tables: `memory_links`, `entities`, `relationships`, `memory_entities`\nNew columns: `valid_from`, `valid_until`, `superseded_by`, `auto_tags`, `keywords`","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-25T15:43:06.623Z","updated_at":"2025-12-25T15:43:06.623Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm4njo6ua","title":"Plugin: Wire memory-tools.ts to real swarm-mail Wave 1-3 adapter","description":"The plugin's memory tools still use a **mock implementation** instead of the real swarm-mail Wave 1-3 adapter.\n\n## Current State\n\n`packages/opencode-swarm-plugin/src/memory.ts`:\n- Line 491-499: \"MOCK implementation for plugin tools\"\n- `upsert()` always returns ADD, doesn't use LLM\n- `store()` doesn't have autoTag/autoLink/extractEntities options\n- Uses local `createMemoryStore` instead of swarm-mail's `createMemoryAdapter`\n\n`packages/opencode-swarm-plugin/src/memory-tools.ts`:\n- `semantic-memory_upsert` has the right args but calls mock implementation\n- `semantic-memory_store` missing autoTag/autoLink/extractEntities args\n\n## Required Changes\n\n### 1. Update memory.ts to use real swarm-mail adapter\n\n```typescript\n// Instead of:\nimport { createMemoryStore } from \"swarm-mail\";\n\n// Use:\nimport { createMemoryAdapter as createSwarmMailAdapter } from \"swarm-mail\";\n```\n\n### 2. Add auto-features to StoreArgs\n\n```typescript\nexport interface StoreArgs {\n  readonly information: string;\n  readonly collection?: string;\n  readonly tags?: string;\n  readonly metadata?: string;\n  readonly confidence?: number;\n  // NEW: Wave 1-3 features\n  readonly autoTag?: boolean;\n  readonly autoLink?: boolean;\n  readonly extractEntities?: boolean;\n}\n```\n\n### 3. Update semantic-memory_store tool\n\n```typescript\nexport const semantic_memory_store = tool({\n  // ... existing args ...\n  autoTag: tool.schema.boolean().optional()\n    .describe(\"Auto-generate tags using LLM\"),\n  autoLink: tool.schema.boolean().optional()\n    .describe(\"Auto-link to related memories\"),\n  extractEntities: tool.schema.boolean().optional()\n    .describe(\"Extract entities for knowledge graph\"),\n});\n```\n\n### 4. Replace mock upsert with real implementation\n\nThe real swarm-mail adapter now has:\n- `upsert()` with LLM-powered ADD/UPDATE/DELETE/NOOP\n- `store()` with autoTag/autoLink/extractEntities options\n- Graceful degradation when LLM unavailable\n\n## Files to Modify\n\n- `packages/opencode-swarm-plugin/src/memory.ts` - Use real adapter\n- `packages/opencode-swarm-plugin/src/memory-tools.ts` - Add new args to store tool\n- `packages/opencode-swarm-plugin/src/memory.test.ts` - Update tests\n\n## Success Criteria\n\n- `semantic-memory_store` supports autoTag/autoLink/extractEntities\n- `semantic-memory_upsert` uses real LLM-powered analysis\n- Graceful degradation when no API key\n- All existing tests pass","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-25T15:43:52.884Z","updated_at":"2025-12-25T15:43:52.884Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjl1ksc3peh","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm824b8ix","title":"Wave 3 Polish: Tests, Docs, Plugin Wiring + Memory Eval ADR","description":"Complete Wave 3 polish work and establish eval foundation for memory system.\n\n## Scope\n1. Fix failing tests (graceful degradation behavior)\n2. Update docs/README for Wave 1-3 features\n3. Wire plugin to real swarm-mail adapter\n4. Write ADR for memory system evals\n5. Add eval scaffolding (one example eval + scorer)\n\n## Context\n- Wave 1-3 memory features complete in swarm-mail\n- Plugin still uses mock implementation\n- 3 tests fail due to behavior change\n- Docs outdated (mention pgvector, missing new features)\n- No evals for LLM-powered memory operations\n\n## Success Criteria\n- All tests pass (195+ pass, 0 fail)\n- README documents Wave 1-3 features\n- Plugin uses real adapter with graceful degradation\n- ADR documents eval strategy for memory system\n- One working eval for smart operations (ADD/UPDATE/DELETE/NOOP)","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-25T15:46:31.739Z","updated_at":"2025-12-25T15:46:31.739Z","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm824gmvu","title":"Fix adapter.test.ts for graceful degradation","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-25T15:46:31.744Z","updated_at":"2025-12-25T15:46:31.744Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjlm824b8ix","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm824iz2s","title":"Update swarm-mail README for Wave 1-3 features","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-25T15:46:31.746Z","updated_at":"2025-12-25T15:46:31.746Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjlm824b8ix","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm824m4d0","title":"Wire plugin memory-tools to real swarm-mail adapter","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-25T15:46:31.750Z","updated_at":"2025-12-25T15:46:31.750Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjlm824b8ix","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm824n37g","title":"ADR: Memory System Eval Strategy","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-25T15:46:31.751Z","updated_at":"2025-12-25T15:46:31.751Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjlm824b8ix","dependencies":[],"labels":[],"comments":[]}
{"id":"opencode-swarm-monorepo-lf2p4u-mjlm824qw2e","title":"Add smart-operations eval with scorer","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-25T15:46:31.754Z","updated_at":"2025-12-25T15:46:31.754Z","parent_id":"opencode-swarm-monorepo-lf2p4u-mjlm824b8ix","dependencies":[],"labels":[],"comments":[]}
